\chapter{Analysis}

This chapter gives an overview over the actual implementation and choices for the formulas in the point source theory chapter.
This should give proper orientation for the steps necessary to reproduce the analysis results.

\begin{itemize}
  \item Describe how HESE maps are build, systematic smearing
  \item Used parameter space
  \item How PDFs and trials are done exactly
  \item Explain details how fitter was used, the seed was found, signal injection, BG injection, BG rate description
  \item Systematic tests: Signal found
  \item Show sensitivity, signal injection, bg injection, ref to MRichman
\end{itemize}


\section{Per event distribution modelling}
\subsection*{Spatial PDF}
Here the spatial signal PDF is modelled using a two dimensional, symmetric Kent distribution, to match the per event uncertainties $\sigma_i$ and a proper normalization on the unit sphere.
The PDF is
\begin{equation}
  f(\Psi|\kappa)
  = \frac{4\pi \sinh\kappa}{\kappa}\exp\left(\kappa(\cos(\Psi)-1)\right)
  \mcomma
\end{equation}
where
\begin{equation}
  \Psi_{i,k}
  = \cos(\delta_k)\cos(\delta_i)\cos(\alpha_k - \alpha_i) +
    \sin(\delta_k)\sin(\delta_i)
\end{equation}
is the space angle between the positions of source $k$ and event $i$ in equatorial coordinates $\delta, \alpha$.
Instead of using the Gaussian uncertainty $\sigma$ directly, the Kent distribution uses $\kappa \approx \sfrac{1}{\sigma^2}$ which holds up to $\sigma \approx \SI{40}{\degree}$ \CITE{relation kappa sigma}\FIXME{plot in appendix?}.
The Kent distribution is used here, because it is correctly normalized on the sphere and virtually indistinguishable for small angle uncertainties as tracks usually have.

Background PDFs are constructed in equatorial coordinates as well and the distribution is estimated from data, using the off time data set only to avoid signal contamination.
By using equatorial coordinates, the right-ascension distribution is assumed to be flat, which translates to a flat distribution in local azimuth coordinates.
This holds well for larger time windows in which the detector rotation relative to the sky smooths out any irregularities in the local azimuth PDF.
For smaller time windows the assumption may break down, and alternatively a local background PDF can be used as a drop in replacement.
Here the PDF is only declination independent and can be written as
\begin{equation}
  f(\delta_i|t_k) = \frac{1}{2\pi}\cdot P(\delta|t_k)
\end{equation}
where $t_k$ is the time of source $k$ used to account for varying background strengths due to seasonal variations \CITE{seasonal variations}\FIXME{explain 1, 2 sentences to seas. var.?}.

To build a custom background PDF for each source in its respective sample, first all events in each sample are binned in $\sin(\delta)$ in $20$ bins, with $14$ more densely spaced bins around the horizon region defined as $\delta\in[\SI{-30}{\degree}, \SI{30}{\degree}]$.
In this region the selection models are usually switched between dedicated models for the northern and southern sky are used and the finer resolution helps to catch the resolution features in the distributions.
To capture the time dependence of the declination dependent background rate, the rate is calculated by using the runtime information from the samples.
Due to a lack of proper run time information, the run lengths are estimates from data by subtracting the earliest from the latest event time per bin \footnote{This overestimates the background rate, because the earliest and latest events can only be close to the real run times, which yields a slightly less performance of this analysis.}.
The rate information is smoothed by re-binning the per run rate bins using a monthly binning.

To get a continuous model for the background expectation per bin, a model
\begin{equation}
  f(t)
  = A\cdot
    \sin\left(\frac{2\pi}{T}\left(t - t_0\right)\right) + R_0
\end{equation}
is fitted to the re-binned time bin centres.
The free parameters are amplitude $A$ and average rate $R_0$ both in $\si{\per\s}$.
For the fit in each bin the period length $T$ is fixed to $365$ days, the natural scale for the seasonal variations and the time offset $t_0$ is fixed from a fit with more statistics using the whole data set \TODO{Add rate plots to app.}.

Having obtained the set of discrete background expectations from the $2$ fit parameters per rate model, a smoothing spline is used to continuously model the fit parameter dependence on declination \TODO{err. scans in the app.?}.
Finally, to get the background PDFs per source, the set of rate model parameters is read off from the built splines on a fine $\sin(\delta)$ grid and plugged into the rate model above.
For each parameter set the model is integrated over the sources time window to obtain an average background PDF per source.
To evaluate the PDf for each events' declination an interpolating spline is used to include the dense grid in a continuous model.

\subsection*{Energy PDF}
The energy PDFs introduces a significant amount of separation power compared to using the spatial clustering only \CITE{Braun Paper}.
The integral values
\begin{equation}
  \int_0^\infty P(E_i,\delta_i|E_\nu)\cdot P(E_\nu|\gamma) \d{E_\nu}
\end{equation}
can be found using simulation if the conditional probabilities $P(E_i,\delta_i|E_\nu)E$ are analytically unknown.
This is done by directly estimating the convolved integral values, here using a two dimensional histogram in $\sin(\delta)$ and in $\log_{10}$ of an energy proxy variable with $30$ equidistant bins between $\lfloor \min\log_{10}(E_\text{proxy}) \rfloor$ and $\lceil \max\log_{10}(E_\text{proxy}) \rceil$.
\TODO{Link to plot in app.}.

To directly obtain the needed signal over background ration $S^E / B^E$ for the test statistic formula, two histograms with the same binning are used, one with using data to obtain the background PDF and the signal one with using signal simulation weighted to the assumed power law with index $\gamma = 2$.
For bins missing either or both data or simulation entries, the ratio is not valid.
To obtain reasonable ratios a two step strategy is used.
First the outermost energy bins per $\sin(\delta)$ column is filled with the lowest, at small energies, or highest, at high energies, ratio value per column.
The invalid ratios are then linearly interpolated from the previously valid and new edge values.
To smooth out unexpected high ratios coming from limited statistics, the ratio is required to monotonically increase in each $\sin(\delta)$ column, which is valid only for the fixed power law and energy range chosen here.
To get a continuous representation of the ratio, a linear, regular grid interpolater is fitted to the ratios.
No extra smoothing e.g. with a Gaussian kernel is applied here.

A more general strategy could be to fill missing values conservatively to lowest non-zero entry per signal and background histogram and then taking the ratio.
To smooth fluctuations a smoothing spline could be fitted to the ratio histogram using the statistical errors as smoothing conditions.
This would also automatically yield analytic derivatives in both axis, which could be handy when fitting more parameters than the single signal strength.
\TODO{Put plots in app.}

\subsection*{Temporal PDF}
The time PDFs are simple rectangle functions
\begin{equation}
  S_{i,k}^T = B_{i,k}^T = T_k(t_i) \coloneqq
    \rect\left(\frac{t_i - \frac{t_k^1-t_k^0}{2}}
                              {t_k^1-t_k^0}\right)
\end{equation}
for both signal and background.
For signal this is a rather generic choice for an unknown emission process.
For background we could more accurately use the same sinus function rate model used for the spatial PDFs.
But as the amplitudes are small, even at the largest time window the resulting PDF would be virtually indistinguishable from a uniform distribution.
So for code simplicity simple rectangle model is used for the background PDF.
This also means that no extra sensitivity is coming from the temporal term, it is merely there to select events within the time windows and alters the amount of total background contribution which is almost zero for the smallest and grows approximately linearly for larger time windows.

\section{Background estimation and stacking weights}
The estimates of the number of background events per source $\Braket{\lambda_{k,B}}$ are not fitted in this analysis, but fixed from a-priori estimates on data.
The values are obtained by fitting the sinus rate model to the whole off time data on the whole sky, again using monthly bins.
\TODO{plots in app.}
The rate model for each sample is then integrated over each sources time window in that sample to obtain the average number of background events.
Again, the amplitudes are quite flat, so the exact integral is almost the same here as just using the product of time window length and average rate.

The a-priori fixed stacking weight should resemble the expected signal emission as closely as possible to the true case, otherwise the analysis' sensitivity will drop significantly.
Because this analysis makes no explicit assumption on the emission model, the weights are computed using a generic power law with index $\gamma=2$.
\FIXME{Explicit nexpected formula with oneweights, connection to point source effA formula.}


\section{Note on LLH minimization}

\section{Trial generation}

\section{HESE reconstruction map handling}
The spatial reconstruction information for the high energy starting events are originally scanned in local detector coordinates zenith and azimuth.
The local event coordinates on the other hand are converted to equatorial coordinates beforehand, because they can be directly compared to source objects that are  usually described in equatorial coordinates \CITE{equ. coords.}.
To become computationally feasible, also the reconstruction maps for the HESE events need to be converted to allow an fast to evaluate equatorial representation.

The used HEALPix maps use an internal coordinate to pixel number conversion scheme, with $\Theta\in[0, \pi]$ and $\varphi\in[0, 2\pi]$, that is easily identifiably with the local detector coordinates zenith $\theta\in[0, \pi]$ and azimuth $\alpha\in[0, 2\pi]$, so the local maps directly represent local coordinates for each pixel \TODO{HEALPix coords in appendix?}.
The conversion to equatorial coordinates depends on the source times which fixes the detector location relative to the equatorial coordinate system.
Due to IceCube's special location almost directly at the geographic South Pole, the relation between zenith $\theta$ and declination $\delta$ angle is $\delta \approx \theta - \sfrac{\pi}{2}$ and only the right-ascension values varies with time.
To avoid recalculating local map coordinates to equatorial coordinates at runtime, pre-transformed maps in equatorial coordinates are computed once beforehand.
The convention used to efficiently map from HEALPix coordinates to equatorial ones is chosen to $\delta = \sfrac{\pi}{2} - \Theta$ and $\alpha = \varphi$.

This mapping is not bijective though, because $\delta \approx \theta - \sfrac{\pi}{2}$ is only an approximation and the number of pixels in each $\Theta$ band changes depending on whether being close to the poles or to the horizon.
So sometimes two pixels are mapped to one, which means that another pixel stays empty, because the number of pixels is fixed.
To overcome this, the mapping is done in reverse by transforming the exact pixel coordinates from a map in equatorial convention back to local coordinates.
Then the local map is interpolated to the new pixel location and that value is stored in the equatorial map.
The maximum error that can happen this way is in the order of a single pixel offset because the above approximation between zenith and declination holds closely enough.

The transformed maps are then converted back to linear PDF space by exponentiating the map and smeared with a one degree symmetric Gaussian kernel to approximately account for unknown systematics.
The smoothing introduces some numerical error because it is done in spherical harmonics space which has to be truncated numerically.
The artefacts are removed by normalizing the smoothed maps to have an integral value of $\sum_{i=1}^{N_\text{pix}} \d{A_\text{pix}} = 1$ over the unit sphere and setting the resulting map to zero outside the $6\sigma$ level.
Due to a lack of a proper test statistic, the likelihood value for the $6\sigma$ level is obtained from Wilks' theorem with
\begin{equation}
  \text{thresh} =
    \max(\mathcal{L}_\text{map})\cdot
    \left(1 - \int_0^{6^2}\chi^2_{2}(x)\d{x}\right)
  \mperiod
\end{equation}
The resulting maps can be used as spatial PDF maps and are sampled for the performance estimation.

\section{Performance estimation}

\section{Post trial method}

