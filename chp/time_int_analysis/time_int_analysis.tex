\chapter{Time integrated analysis}

Another analysis was performed to test for a potential steady flux scenario from the HESE event positions.
This may be seen as the special case to the previous analysis using time windows as large as all samples combined across all sources.

\FIXME{Physics introduction and reference to blazar stackings and 7yr PS search}

\FIXME{Short overview over the method and differences to the time dep ana.}
The method used in this analysis is similar to the previous one and uses the same Likelihood mechanism as derived in the dedicated chapter using different approximations suitable for large statistics samples and a steady source scenario.
The fact that all sources are assumed to contribute to the common neutrino flux at all times throughout all samples simplifies the construction of background PDFs and the process of trial generation.
Nevertheless the background PDFs are still constructed from the data sample.
In this case, this is justified by the assumption of having a low amount of signal in the sample and the spatial scrambling of the event positions in right-ascension to ensure any potential signal clustering is removed.
Trials are also generated by using the full set of background trials, also spatially scrambled in each trial.
The main difference to the time dependent analysis is the usage of a variable unbroken power law spectrum in the energy PDF.
This introduces a second free fit parameter, namely the spectral index of the assumed power law, equal for all sources.
This is necessary because with the larger statistics, the small time window effect are removed and the test statistics are more comparable as expected from Wilks' theorem.
This however makes it necessary to properly sample the signal PDF in trial generation and to give the fitting procedure the chance to properly describe different realisations of emerging spectra.

\section{Per event distribution modelling}
While in general the method to generate probability distributions from measured data and simulation, the procedure here has some differences to be compliant to previous IceCube point source analysis, using the same data sets for time integrated searches.
A set of rules regarding the binning and handling of these data tuned for optimal result in time integrated searches is therefore also used in this analysis and shortly described below.

\subsection*{Spatial PDF}
The spatial signal PDF is defined as a two dimensional gaussian for comparability reasons in this case.
However this makes virtually no differences to the previous definition where the Kent distribution was used.
As the data set is made of well reconstructed muon tracks, the angular uncertainty is small enough to avoid the regions in which both PDFs would start to severely differ \TODO{ref to kent vs gaus plot?}.

The background PDF construction is similar but simpler here as only a single PDF is required to describe the background behaviour.
As seasonal variations are smoothed out in the whole sample livetime, it is sufficient to construct a single distribution from data
\begin{equation}
  f(\delta_i) = \frac{1}{2\pi}\cdot P(\delta_i)
  \mperiod
\end{equation}
This is done by histogramming the data events in sine of declination and fitting an interpolating spline through the resulting bin centre points to have a smooth and continuous representation of the PDF available.
Again for being consistent with the commonly used PDFs in IceCube, the binning is chosen a bit different than in the previous analysis, which makes almost no difference in the used PDF \TODO{compariosn plot?}.
The binning set-up can be looked up in the \lstinline!skylab! \CITE{skylab} module which collects the shared settings for the datasets.

\subsection*{Energy PDF}
The energy PDF needs to be build evaluated for multiple power laws at runtime because the second fit parameter is the spectral index of the assumed source flux.
To avoid long run times by recomputing the PDF for every occurring index, the energy PDF is built in three instead of 2 dimension.

For a fixed index $\gamma$ the histograms are built as before from data and signal simulation with the same binning and the ratio built to describe the signal over background ration for the energy term.
For signal simulation the number of events per bin can be computed using the modified OneWeight
\begin{equation}
  N(\Delta E, \Delta\delta)
  = T\cdot \sum_{(i|E_{\nu,i}\in\Delta E, \delta_i\in\Delta\delta)}
    \frac{\text{OneWeight}'_i \cdot \phi(E_i|\gamma)}{\Delta\delta \Delta E}
  \mcomma
\end{equation}
where now the spectral index $\gamma$ can vary for each source flux hypothesis and each sample livetime needs to regarded to adapt the expected number of signal events accordingly.
The slightly different binning can again be looked up in the \lstinline!skylab! module.


\section{Background estimation and stacking weights}
Weights are same, but discuss differences in binning


\section{Note on LLH minimization}
Maybe discuss derivative in ns and gamma?


\section{Trial generation}
Discuss differences

\subsection*{Background trials}
Trials are much simpler made by scrambling background

\subsection*{Signal trials}
Discuss difference fluence burst mode vs. constant flux injection.
Mention, that chi2 method is the same.
Use skylab module or adapt own healpy injector?


\section{Results}
