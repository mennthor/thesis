\chapter{Time-integrated analysis}
  \label{chp:time_indep}
Another analysis was performed to test for a potential steady flux scenario from the HESE event positions.
This may be seen as the special case to the previous analysis in chapter~\ref{chp:time_dep} using time windows as large as all samples combined across all sources.
The computing part is done using the \lstinline!skylab! \CITE{skylab, coenders phd} module which is a versatile tool for performing various types of time-integrated point source searches in IceCube.
Therefore the technical details of the implementation can be reviewed in the module description and only a short summary focusing on the key differences to the time-dependent search is given in this chapter.

As mentioned before, no significant excess of neutrinos at any location on the sky could be found in the extensive all-sky search with seven years of data.
The results also indicate, that at each HESE position no significant source can be identified.
This motivates to conduct a time-independent search independently of the previous time-dependent one, to check for the possibility that possible sources that may have emitted the HESE events are only yielding enough events in a combined search.

The method used in this analysis is similar to the previous in chapter~\ref{chp:time_dep} and uses the same underlying Likelihood mechanism.
The detailed derivation of the used formula using different approximations suitable for large statistics samples and a steady source scenario can be seen in the dedicated section~\ref{chp:pointsource_time_int_llh}.
The fact that all sources are assumed to contribute to the common neutrino flux at all times throughout all samples simplifies the construction of background PDFs and the process of trial generation.
Therefore no splitting in off- and on-data regions is done beforehand.
By scrambling all data events in right-ascension before building any PDFs or doing trials, the experimental data is kept \enquote{blinded}.

The main difference to the time-dependent analysis is the usage of a variable, unbroken power law spectrum in the energy PDF.
This introduces a second free fit parameter, namely the spectral index $\gamma$ of the assumed power law flux $E_\nu^{-\gamma}$ shared between all sources under the assumption of equal emission scenarios.
Due to the much larger statistics, it is possible to reliably fit another parameter.
From a technical point of view, the new degree of freedom is also needed to match the model to background and signal assumptions.
While the specific shape of the underlying spectral model was not very important in the time-dependent analysis due to the low overall background expectation, it is necessary here to distinguish signal and background contributions more efficiently.

Performance estimation is done exactly as in the time-dependent analysis, by scanning a grid of true source strengths and building a test statistic for each grid point.
This yields the full Neyman plane and can be used to fit a fairly general $\chi^2$ CDF to the samples, building the required performance parameters or limits.
No post trials are needed here because no grid scan is made.
Instead, the spectral index is fitted, which incorporates the trial factor automatically in the fitting procedure.


\section{Per event distribution modelling}
While in general, the method to generate probability distributions from measured data and simulation, the procedure here has some differences to be compliant to previous IceCube point source analysis, using the same datasets for time-integrated searches.
A set of rules regarding the binning and handling of these data, tuned for optimal results in time-integrated searches is therefore also used in this analysis and shortly described below.

\subsection*{Spatial PDF}
The spatial signal PDF is defined as a two-dimensional Gaussian distribution for comparability reasons in this case.
However, this makes virtually no differences to the previous definition where the Kent distribution was used.
As the dataset consists of well-reconstructed muon tracks, the angular uncertainty is small enough to avoid the regions in which both PDFs would start to severely differ.

The background PDF construction is similar but simpler here, as only a single PDF is required to describe the background behaviour.
Because seasonal variations are averaged out in the whole sample livetime, it is sufficient to construct a single average distribution from data
\begin{equation}
  f(\delta_i) = \frac{1}{2\pi}\cdot P(\delta_i)
  \mperiod
\end{equation}
This is done by histogramming the data events in sine of declination and fitting an interpolating spline through the resulting bin centre points to have a smooth and continuous representation of the PDF available.
Again for being consistent with the commonly used PDFs in IceCube, the binning is chosen a bit different than in the previous analysis, which makes almost no difference in the used PDF \FIX{compariosn plot?}.
The binning set-up can be looked up in the \lstinline!skylab! module which collects the shared settings for the datasets.

\subsection*{Energy PDF}
The energy PDF needs to be built and evaluated for multiple power law spectral indices because the second fit parameter is the spectral index of the assumed source flux.
To avoid long computing times by re-calculating the PDF for every occurring index, the energy PDF is built in three instead of two dimensions and cached beforehand.

For a fixed index $\gamma$ the histograms are built as before from data and signal simulation with the same binning and the ratio built to describe the signal over background ration for the energy term.
For simulated signal data, the number of events per bin can be computed using the modified OneWeight \footnote{See equation~(\ref{equ:oneweight_definition}) for a more detailed description of OneWeight.}
\begin{equation}
  N(\Delta E, \Delta\delta)
  = T\cdot \sum_{(i|E_{\nu,i}\in\Delta E, \delta_i\in\Delta\delta)}
    \frac{\text{OneWeight}'_i \cdot \phi(E_i|\gamma)}{\Delta\delta \Delta E}
  \mcomma
\end{equation}
where now the spectral index $\gamma$ can vary for each source flux hypothesis and each sample livetime $T$ needs to be regarded to adapt the expected number of signal events for each sample accordingly.
The slightly different binning can again be looked up in the \lstinline!skylab! module.

To cache the energy PDF dependency on the spectral index, the signal over background ratios are built for each $\gamma$ on a grid, which is constrained in the interval $[1, 4]$ to cover a broad range of physically plausible hard and soft energy spectra.
The stack of histograms is then used to create a one-dimensional spline in the $\gamma$ direction to obtain a continuous description and get the needed derivatives for the fit in analytic form as well.
The built splines are cached during runtime to avoid repeated construction for the same declinations and energies.
\TODO{Example hists like on GFU wiki}.
A less complicated method would be to use multi-dimensional, splines directly to parametrize the PDF.
In this case, the spline would be three-dimensional, in energy, $\sin(\delta)$ and $\gamma$,
The advantage would be to have a single description of the PDF and obtain analytic gradients in all directions automatically.


\section{Stacking and multi-sample weights}
Here the source weights and the multi-sample $n_S$ split weights both depend on the current spectral index and have to be re-evaluated each time the index changes.
Otherwise, the calculation happens similar to the procedure in the time-dependent analysis, but the signal simulation needs to respect the sample livetime $T$ and the current spectral index $\gamma$ for the assumed power-law flux $\phi$ in the weights
\begin{equation}
  w_i = T\cdot \phi(E_{\nu,i}|\gamma)\cdot\text{OneWeight}'_i
  \mperiod
\end{equation}
An interpolating spline is used to continuously describe the histogram, normalized by the bin width $\Delta\Omega_\text{bin}$ again.
Though using the same spline per sample, the difference in stacking and sample split weights is the normalization procedure.
Stacking weights only consider how a given signal is split according to the expectation within one sample and are normalized accordingly
\begin{equation}
  \sum_{k=1}^{N_\text{src}} w_{k,j} = 1
  \mcomma
\end{equation}
whereas the $n_S$ sample split weights $w_j$ follow the normalization rule from the aforementioned law of total probabilities
\begin{equation}
  \sum_{j=1}^{N_\text{sam}} w_j
  = \sum_{j=1}^{N_\text{sam}} \sum_{k=1}^{N_\text{src}} P(j|k)\cdot P(k) = 1
  \mcomma
\end{equation}
where $P(j|k)$ and $P(k)$ can again be obtained by the number of expected events depending on the current spectral index and have to be adapted accordingly during a fitting procedure.
\TODO{Gamma vs ns split weights plot for a sample}


\section{Note on LLH minimization}
The extra fit parameter $\gamma$ makes it necessary to calculate an additional analytic gradient $\partial{\gamma}$ to avoid costly Likelihood evaluations for numeric gradient calculations and more exact gradients.
Coming from the multi-sample Likelihood, the gradients are
\begin{align}
  \deldel{(-2\ln\Lambda)}{n_S}
  &= \deldel{}{n_S}\left\{
      2\sum_{j=1}^{N_\text{sam}} \left[
        \sum_{i=1}^{N_j} \ln\left(
          \frac{n_S}{N_j}\left( R_{i,j} - 1 \right) + 1
        \right)
      \right]
    \right\} \\
  &= 2\sum_{j=1}^{N_\text{sam}} \left[
      \sum_{i=1}^{N_j}
        \frac{R_{i,j} - 1}{n_S\left( R_{i,j} - 1 \right) + N_j}
      \right] \\
  \intertext{for $n_S$ and}
  \deldel{(-2\ln\Lambda)}{\gamma}
  &= \deldel{}{\gamma}\left\{
      2\sum_{j=1}^{N_\text{sam}} \left[
        \sum_{i=1}^{N_j} \ln\left(
          \frac{n_S}{N_j}\left( R_{i,j} - 1 \right) + 1
        \right)
      \right]
    \right\} \\
  &= 2\sum_{j=1}^{N_\text{sam}} \left[
      \sum_{i=1}^{N_j}
        \frac{n_S}{n_S\left( R_{i,j} - 1 \right) + N_j}\cdot
        \deldel{R_{i,j}}{\gamma}
      \right] \\
\end{align}
for $\gamma$ respectively, where
\begin{equation}
  R_{i,j} = R_{i,j}(\gamma)
  \coloneqq \frac{\sum_{k=1}^{N_\text{srcs}}(w_j(\gamma)
                  w_{k,j}(\gamma) S_{i,k,j}(\gamma))}{B_{i,j}}
\end{equation}
was introduced for a shorter notation.
The $\partial{\gamma}$ derivatives of the signal over background ratios $R_{i,j}$ can be evaluated using the analytic spline derivatives used to construct the  weights and PDFs and the application of the product derivation rule
\begin{equation}
  \deldel{R_{i,j}}{\gamma}
  = \frac{1}{B_{i,j}}\cdot\sum_{k=1}^{N_\text{srcs}}\left(
      \deldel{w_j}{\gamma} w_{k,j} S_{i,k,j} +
      w_j \deldel{w_{k,j}}{\gamma} S_{i,k,j} +
      w_j w_{k,j} \deldel{S_{i,k,j}}{\gamma}
    \right)
  \mperiod
\end{equation}


\section{Trial generation}
Trial generation is similar to the method in the time-dependent analysis.
The main differences here are the easier background trial generation, by using the full set of spatially scrambled background data and the different signal injection due to the time-integrated flux.

Here the added fit parameter $\gamma$ leads to a better conversion to a $\chi^2$ background only test statistic as expected from Wilks' theorem.
Using a fixed index would result in many test statistic values of zero during background trials because a larger set of background events clearly follows a much steeper energy spectrum as predicted signal and thus makes it hard to properly sample a rather flat spectrum from the given experimental dataset.
The variable index results in almost $\SI{50}{\percent}$ over- and under-fluctuations in background trials with spectral indices almost always fitted in the steep spectrum region $\gamma_\text{BG}\in[3.5, 4.0]$ \TODO{Backup with gamma hist?}.

\subsection*{Background trials}
Pure background pseudo data samples are still generated by using the experimental data in this analysis for the same reason of avoiding bias from mismatches between data and background simulation.
For each trial, a new right-ascension value is assigned to scramble the events.
The usage of experimental data as background is justified by the assumption of having a low amount of signal in the sample and that the spatial scrambling of the event positions in right-ascension ensures the removal of any potential signal clustering.
Additionally, because the Likelihood works with the approximation of having a constant number of events by dropping the Poisson term, the number of background events stays constant in each trial.

$\num{e6}$ background only trials where performed to sufficiently describe the test statistic distribution.
Again, no under-fluctuations are fitted and the fit parameter $n_S$ is truncated at zero.
Due to the variable index, this is not as large an issue as described above.
The statistic follows the expected $\chi^2$ distribution well and is therefore described with a model from a common approach, which is to fit the expected $\chi^2$ distribution directly.
Despite expecting the number of degrees of freedom to be one, it is slightly higher.
This occurs because at exactly $\SI{100}{\percent}$ pure background, which means $n_S=0$, the spectral index has no relevance any more and is degenerate, leading to a slightly different effective number of degrees of freedom $\hat{n}$.
The PDF fitted to the test statistic is then a split PDF catching the compactified under-fluctuations at zero and the tail with a $\chi^2$ distribution \FIX{Plot chi2 vs. exp tail?}
\begin{equation}
  f(x \coloneqq -2\ln\hat{\Lambda}) =
  \begin{cases}
    (1 - \eta) \cdot \chi^2_{\hat{n}}(x) &\text{ , for}\quad x > 0 \\
    \eta &\text{ , for}\quad x = 0
  \end{cases}
  \mperiod
\end{equation}
% The resulting fit and a comparison to the previously used empirical test statistic approach can be seen in figure~(\ref{fig:time_int_bg_ts}).

% \begin{figure}[htbp]
%   \centering
%   \subimport*{plots/}{bg_ts.pgf}
%   \caption{
%     Background only test statistic with both the exponential tail model and the direct $\chi^2$ model.
%     The $\chi^2$ model is slightly more conservative and is used here to obtain the final p-value on the experimental data result.
%     For the exponential tail the threshold from where the empirical part is used is indicated by the dashed vertical line at a test statistic value of $\num{5.15}$.
%   }
%   \label{fig:time_int_bg_ts}
% \end{figure}


\subsection*{Signal trials}
The signal injection is set up to simulate a steady flux scenario.
This comes with two main differences to the time-dependent part.
First, all sources contribute in every sample and are no longer unique and only present in a single sample.
Second, the expected flux is directly proportional to the data livetime in each sample.
Both properties need to be reflected by the signal injector.

\begin{equation}
  w_{i,k} = w_k \frac{\text{OneWeight}'_i \cdot
    \Phi(E_{\nu,i})}{\Omega_k} \cdot T
  \mperiod
\end{equation}
where $k$ is the source the event was selected at, $w_k$ the combined source weight and $T$ the sample livetime in seconds.
The simulated diffuse flux $\phi(E_{\nu,i})$ is normalized to a point source flux by dividing out the solid angle $\Omega_k$ of the selected regions around each source and has units $\si{\per\GeV\per\cm\squared\per\second}$.

The sampling itself stays the same and is also used with the same HEALPix injection mode to account for the source position uncertainties.
Timing information is not sampled because it is not needed any more.

To combine multiple signal injectors, the distribution weights need to be adapted to account for the fact, that all sources are present in every sample.
This scenario is actually simpler to handle and it is sufficient to distribute the samples according to their expected events in total, without a need to re-normalize the source weights.


\section{Performance}
\FIX{Present fluxes for sensitivity and discovery potential total and per source}


\section{Results}
The result is obtained by doing a single Likelihood fit on unscrambled experimental data.
The resulting test statistic is compared to the background only test statistic model to obtain the final p-value

No significant excess of neutrinos is seen in the test for a steady state flux scenario.
The best-fit test statistic is $\num{0.057}$ with best-fit parameters $\hat{n}_S = \num{5.57}$ and $\hat{\gamma} = \num{2.8}$.
This complies to a p-value of $p=0.47$ and a significance of $\SIsigma{0.73}$.
The  best fit test statistic value together with the background only distribution can be seen in figure~(\ref{fig:time_int_bg_ts_best_fit}).

% \begin{figure}[htbp]
%   \centering
%   \includegraphics{plots/bg_ts_best_fit}
%   % \subimport*{plots/}{bg_ts_best_fit.pgf}
%   \caption{
%     Background only test statistic together with the experimental, unblinded result.
%     The best test statistic value of $\num{0.057}$ was shifted to twice its value to be visible in the plot.
%     In solid, the fitted $\chi^2$ PDF model which is used to estimate the p-value is overlaid.
%   }
%   \label{fig:time_int_bg_ts_best_fit}
% \end{figure}

