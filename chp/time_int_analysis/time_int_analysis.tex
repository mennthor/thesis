\chapter{Time-integrated analysis}
  \label{chp:time_indep}
In addition to the time-dependent analysis in the previous chapter~\ref{chp:time_dep}, a time-integrated analysis is performed to test for a potential steady flux scenario from the high energy starting event positions.
This may be seen as the special case to the previous analysis using time windows as large as all samples combined across all sources.
Instead of using a self-developed computing code, the calculations are mostly done using the \lstinline!skylab! \cite{Aartsen:2016oji,skylabrepo} module here, which is a versatile tool for performing various types of time-integrated point source searches in IceCube.
Therefore the technical details of the implementation can be reviewed in the software description and only a short summary focusing on the key differences to the time-dependent search is given in this chapter.
For the signal injection, the same injection code as used for the time-dependent analysis is used via a small adapter to be able to take the source position uncertainties into account with the \lstinline!skylab! code.

As mentioned before, no significant excess of neutrinos at any location on the sky could be found in the extensive all-sky search with seven years of data.
The results also indicate, that at each HESE position no significant source can be identified.
This motivates to additionally conduct a time-independent stacked search independently of the previous time-dependent one, to check for the possibility that the sources, that may have emitted the high energy starting events, are only emitting enough events for a significant detection in a combined search.

The method used in this analysis is similar to the previous one and uses the same underlying Likelihood mechanism.
The detailed derivation of the used formula using different approximations suitable for large statistics samples and a steady source scenario can be seen in the dedicated section~\ref{chp:pointsource_time_int_llh}.
The fact that all sources are assumed to contribute to the common neutrino flux at all times throughout all samples simplifies the construction of background PDFs and the process of trial generation.
Therefore no splitting in off- and on-data regions is done beforehand.
By scrambling all data events in right ascension before building any PDFs or doing trials, the experimental data is kept \enquote{blinded}.

The main difference to the time-dependent analysis is the usage of a variable, unbroken power law spectrum in the energy PDF.
This introduces a second free fit parameter, namely the spectral index $\gamma$ of the assumed power law flux $\sim E^{-\gamma}$ shared between all sources under the assumption of equal intrinsic emission.
Due to the much larger statistics, it is possible to reliably fit the additional parameter.
From a technical point of view, the new degree of freedom is also needed to match the model to background and signal assumptions.
While the specific shape of the underlying spectral model was not very important in the time-dependent analysis due to the low overall background expectation, it is necessary here to distinguish signal and background contributions more efficiently.

Performance estimation is done exactly as in the time-dependent analysis, by scanning a grid of true source strengths and building a test statistic for each grid point.
This yields the Neyman plane marginalized over $\gamma$ and can be used to fit a fairly general $\chi^2$ CDF to the samples, to build the required performance parameters or limits.
Also differential performances are calculated for a quite high resolution to allow for the computation of a wide range of custom models.
No post trials are needed here because no grid scan is made.
Instead, the spectral index is fitted directly, which incorporates the trial factor automatically in the fitting procedure.


\section{Per event distribution modelling}
The methods to generate probability distributions from measured data and simulation, are chosen to be compliant to previously conducted IceCube point source analysis, which used the same or similar datasets for time-integrated searches.
A set of rules regarding the binning and handling of these data, tuned for optimal results in time-integrated searches is therefore also used in this analysis and shortly described below.

\subsection*{Spatial PDF}
Here, the spatial signal PDF is defined as a two-dimensional Gaussian distribution.
However, this makes virtually no differences to the previous signal PDF, for which where the Kent distribution was used.
As the dataset consists of well-reconstructed muon tracks, the angular uncertainty is small enough to avoid the regions in which both PDFs would start to severely differ.

The background PDF construction is similar to the time-dependent ones, but is simpler to compute here.
Because seasonal variations are averaged out in the whole sample livetime, it is sufficient to construct a single average distribution from data to describe the background behaviour
\begin{equation}
  f(\delta_i) = \frac{1}{2\pi}\cdot P(\delta_i)
  \mperiod
\end{equation}
This is done by histogramming the data events in sine of declination and fitting an interpolating spline through the resulting bin centre points to have a smooth and continuous representation of the PDF available.
Again for being consistent with the commonly used PDFs in IceCube, the binning is chosen a bit different than in the previous analysis, which makes almost no difference in the used PDF.
The binning set-up can be looked up in the \lstinline!skylab! module which collects the shared settings for the datasets.

\subsection*{Energy PDF}
The energy PDF needs to be built and evaluated for multiple power law spectral indices because the second fit parameter is the spectral index of the assumed source flux.
To avoid long computing times by re-calculating the PDF for every occurring index, the energy PDF is built in three instead of two dimensions and cached beforehand.
To avoid long computing times by re-calculating the two-dimensional PDFs in $\sin(\delta)$ and $\log_{10}\left(E_\text{proxy}\right)$ for every index $\gamma$ occurring during the fitting procedures, the PDFs are built and cached for a fine grid of spectral indices beforehand.

For a fixed index $\gamma$ the histograms are built as before from data and signal simulation with the same binning and the ratio built to describe the signal over background ration for the energy term.
For simulated signal data, the number of events per bin can be computed using the modified OneWeight \footnote{See equation~(\ref{equ:oneweight_definition}) for a more detailed description of OneWeight.}
\begin{equation}
  N(\Delta E, \Delta\delta)
  = T\cdot \sum_{(i|E_{\nu,i}\in\Delta E, \delta_i\in\Delta\delta)}
    \frac{\text{OneWeight}'_i \cdot \phi(E_i|\gamma)}{\Delta\delta \Delta E}
  \mcomma
\end{equation}
where now the spectral index $\gamma$ can vary for each source flux hypothesis and each sample livetime $T$ needs to be regarded to adapt the expected number of signal events for each sample accordingly.
The slightly different binning can again be looked up in the \lstinline!skylab! module.

To cache the energy PDF dependency on the spectral index, the signal over background ratios are built for each $\gamma$ on a grid, which is constrained in the interval $[1, 4]$ to cover a broad range of physically plausible hard and soft energy spectra.
The stack of histograms is then used to create a one-dimensional spline in the $\gamma$ direction on the fly for each pair of $\left(\sin(\delta), \log_{10}\left(E_\text{proxy}\right)\right)$ during the fit, to obtain a continuous description and get the needed derivatives for the fit in analytic form as well.
The built splines are cached during runtime to avoid repeated construction for the same declinations and energies.
A less complicated method would be to use multi-dimensional, splines directly to parametrize the PDF.
In this case, the spline would be three-dimensional, in energy $\log_{10}\left(E_\text{proxy}\right)$, $\sin(\delta)$ and $\gamma$.
The advantage would be to have a single description of the PDF and obtain analytic gradients in all directions automatically.
However, it might be more difficult to verify a proper description of the underlying data for higher dimensions, so a thorough verification process would be needed.
Alternatively, a multidimensional kernel density estimation could be considered, which would automatically come with an easy possibility to directly sample the distributions for trial generation.


\section{Stacking and multi-sample weights}
Here the source weights and the multi-sample $n_S$ split weights both depend on the current spectral index and have to be re-evaluated each time the index changes.
Otherwise, the calculation happens similar to the procedure in the time-dependent analysis, but the signal simulation needs to respect the sample livetime $T$ and the current spectral index $\gamma$ for the assumed power-law flux $\phi$ in the weights
\begin{equation}
  w_i = T\cdot \phi(E_{\nu,i}|\gamma)\cdot\text{OneWeight}'_i
  \mperiod
\end{equation}
An interpolating spline is used to continuously describe the histogram, normalized by the bin width $\Delta\Omega_\text{bin}$ again.
Though using the same spline per sample, the difference in stacking and sample split weights is the normalization procedure.
Stacking weights only consider how a given signal is split according to the expectation within one sample and are normalized accordingly
\begin{equation}
  \sum_{k=1}^{N_\text{src}} w_{k,j} = 1
  \mcomma
\end{equation}
whereas the $n_S$ sample split weights $w_j$ follow the normalization rule from the aforementioned law of total probabilities~(\ref{equ:law_of_total_probability})
\begin{equation}
  \sum_{j=1}^{N_\text{sam}} w_j(\gamma)
  = \sum_{j=1}^{N_\text{sam}} \sum_{k=1}^{N_\text{src}}
      P(j|k,\gamma)\cdot P(k|\gamma) = 1
  \mcomma
\end{equation}
where $P(j|k,\gamma)$ and $P(k|\gamma)$ can again be obtained by the number of expected events depending on the current spectral index $\gamma$ and have to be adapted accordingly during a fitting procedure.


\section{Note on LLH minimization}
The extra fit parameter $\gamma$ makes it necessary to calculate an additional analytic gradient $\partial{\gamma}$ to avoid costly Likelihood evaluations for numeric gradient calculations and to obtain precise gradients.
For the full multi-sample Likelihood~(\ref{equ:multi_sam_time_indep}), the gradients are
\begin{align}
  \deldel{(-2\ln\Lambda)}{n_S}
  &= \deldel{}{n_S}\left\{
      2\sum_{j=1}^{N_\text{sam}} \left[
        \sum_{i=1}^{N_j} \ln\left(
          \frac{n_S}{N_j}\left( R_{i,j} - 1 \right) + 1
        \right)
      \right]
    \right\} \\
  &= 2\sum_{j=1}^{N_\text{sam}} \left[
      \sum_{i=1}^{N_j}
        \frac{R_{i,j} - 1}{n_S\left( R_{i,j} - 1 \right) + N_j}
      \right] \\
  \intertext{for $n_S$ and}
  \deldel{(-2\ln\Lambda)}{\gamma}
  &= \deldel{}{\gamma}\left\{
      2\sum_{j=1}^{N_\text{sam}} \left[
        \sum_{i=1}^{N_j} \ln\left(
          \frac{n_S}{N_j}\left( R_{i,j} - 1 \right) + 1
        \right)
      \right]
    \right\} \\
  &= 2\sum_{j=1}^{N_\text{sam}} \left[
      \sum_{i=1}^{N_j}
        \frac{n_S}{n_S\left( R_{i,j} - 1 \right) + N_j}\cdot
        \deldel{R_{i,j}}{\gamma}
      \right]
\end{align}
for $\gamma$ respectively, where
\begin{equation}
  R_{i,j} = R_{i,j}(\gamma)
  \coloneqq \frac{\sum_{k=1}^{N_\text{srcs}}(w_j(\gamma)
                  w_{k,j}(\gamma) S_{i,k,j}(\gamma))}{B_{i,j}}
\end{equation}
was introduced for a shorter notation.
The $\partial{\gamma}$ derivatives of the signal over background ratios $R_{i,j}$ can be evaluated using the analytic spline derivatives used to construct the  weights and PDFs and the application of the product derivation rule
\begin{equation}
  \deldel{R_{i,j}}{\gamma}
  = \frac{1}{B_{i,j}}\cdot\sum_{k=1}^{N_\text{srcs}}\left(
      \deldel{w_j}{\gamma} w_{k,j} S_{i,k,j} +
      w_j \deldel{w_{k,j}}{\gamma} S_{i,k,j} +
      w_j w_{k,j} \deldel{S_{i,k,j}}{\gamma}
    \right)
  \mperiod
\end{equation}


\section{Trial generation}
Trial generation is similar to the method in the time-dependent analysis.
The main differences here are the easier background trial generation, by using the full set of spatially scrambled background data and the different signal injection due to the time-integrated flux.

Here the added fit parameter $\gamma$ leads to a better conversion to a $\chi^2$ background only test statistic as expected from Wilks' theorem.
Using a fixed index would result in many test statistic values of zero during background trials because a larger set of background events follows a much steeper energy spectrum as predicted signal and thus makes it hard to properly sample a rather flat spectrum from the given experimental dataset.
The variable index results in almost $\SI{50}{\percent}$ over- and under-fluctuations in background trials with spectral indices preferably getting fitted in the more steep spectral region with indices $\gamma_\text{BG}\in[2.7, 3.7]$.
These are shifted a bit from the expected atmospheric index of about $\num{3.7}$ indicating a small bias in the fit parameter from the background scrambles.

\subsection*{Background trials}
Pure background pseudo data samples are generated by using the experimental data in this analysis for the same reason of avoiding bias from mismatches between data and background simulation.
To obtain a pseudo background sample, it is sufficient to simply assign a new right ascension vale, sampled uniformly in $[0, 2\pi]$ to each event per trial.
The usage of experimental data as background is justified by the assumption of expecting a low amount of signal in the sample and that the spatial scrambling of the event positions in right ascension ensures the removal of any potential signal clustering.
Additionally, because the Likelihood works with the approximation of having a constant number of events by dropping the Poisson term, the number of background events stays constant in each trial.

$\num{e6}$ background only trials where performed to describe the test statistic distribution sufficiently accurate.
Again, no under-fluctuations are fitted and the fit parameter $n_S$ is truncated at zero.
Due to the variable index, this is not as large an issue as previously described.
The test statistic follows the expected $\chi^2$ distribution well and is therefore described with a modified $\chi^2$ PDF, which is the common choice for a time-integrated search.
The number of degrees of freedom of the $\chi^2$ PDF turns out to be slightly higher than its expected value of $\num{1}$ from Wilks' theorem.
This is because for very background-like samples, which means $n_S=0$, the spectral index of the signal energy PDF has no relevance any more and is degenerate, leading to the slightly different effective number of degrees of freedom $\hat{n}$.
The distribution fitted to the test statistic is a split PDF model catching the compactified under-fluctuations at zero with a delta peak and the tail with a $\chi^2$ distribution
\begin{equation}
  f(x \coloneqq -2\ln\hat{\Lambda}) =
  \begin{cases}
    (1 - \eta) \cdot \chi^2_{\hat{n}}(x) &\text{ , for}\quad x > 0 \\
    \eta &\text{ , for}\quad x = 0
  \end{cases}
  \mperiod
\end{equation}
The PDF is fitted to the test statistic by only varying the effective number of degrees of freedom $\hat{n}$.
The number of zero trials $\eta$ is directly deduced by simple counting.

\subsection*{Signal trials}
The signal injection is set up to simulate a steady flux scenario.
This comes with two main differences to the time-dependent part.
First, all sources contribute in every sample and are no longer unique and only present in a single sample.
Second, the expected flux is directly proportional to the data livetime in each sample.
Both properties need to be reflected by the signal injector.
The weights for the signal event sampling are constructed with
\begin{equation}
  w_{i,k} = w_k \frac{\text{OneWeight}'_i \cdot
    \phi(E_{\nu,i})}{\Omega_k} \cdot T
  \mperiod
\end{equation}
where $k$ is the source the event was selected at, $w_k$ the combined source weight and $T$ the sample livetime in seconds.
The simulated diffuse flux $\phi(E_{\nu,i})$ is normalized to a point source flux by dividing out the solid angle $\Omega_k$ of the selected regions around each source and has units $\si{\per\GeV\per\cm\squared\per\second}$.
The sampling itself stays the same and is also used with the same HEALPix injection mode to account for the source position uncertainties.
No time information is sampled or needed in this steady-state scenario.

To combine multiple signal injectors, the distribution weights need to be adapted to account for the fact, that all sources are present in every sample.
Here, it is sufficient to distribute the samples according to their expected events in total, without a need to re-normalize the source weights
\begin{equation}
  w_j = \frac{N_j}{\sum_{m=1}^{N_\text{sam}}N_m}
  \mperiod
\end{equation}


\section{Performance}
The a priori estimation of the analysis performance is performed analogue to the procedure in section~\ref{chp:time_dep_perf}.
Because the performance flux is just a single value for the full analysis, a finely binned differential performance is computed here.
The differential performance is estimated in logarithmic energy bins, uniformly spaced with an eighth of a decade between $\SI{e2}{\GeV}$ and $\SI{e9}{\GeV}$.
Additionally the differential performance is computed for two injection spectra, with indices $\gamma_\text{inj} = 2$ and $\gamma_\text{inj} = 3$ to validate if the bin size is sufficiently small to justify the creation of arbitrary flux performances and limits as explained in section~\ref{chp:time_dep_diff_perf}.

The connection between the flux normalisation $\phi_0$ and the model flux $F$ is slightly different here, because of the different emission scenario
\begin{align}
  N
  &= T\cdot\sum_{k=1}^{N_\text{srcs}} \sum_{(i|\Omega_i \in \Omega_k)}
      w_k\frac{\text{OneWeight}'_i\cdot\phi_0\cdot f(E_{\nu,i})}{\Omega_k} \\
  &= \phi_0 \cdot
      \sum_{k=1}^{N_\text{srcs}}
      \sum_{(i|\Omega_i \in \Omega_k)} w'_{k,i}
  \coloneqq \phi_0 \cdot \sum_{k=1}^{N_\text{srcs}} F_k = \phi_0 \cdot F
  \mcomma
\end{align}
where $T$ is the sample livetime.
A global flux normalization $\phi'_0$ can equivalently be obtained from the differential performance normalizations $\phi_j^0$ using
\begin{equation}
  \phi'_0 = \frac{1}{\sum_{j=1}^{N_\text{bins}} w_j}
\end{equation}
where the weights $w_j$ are computed from
\begin{equation}
  w_j = \frac{\int_{\Delta E_j} f'(E)\,\text{d}E}
             {\phi_j^0\cdot\int_{\Delta E_j} f(E)\,\text{d}E}
  \mperiod
\end{equation}


\section{Results}
The result is obtained by doing a single Likelihood fit on unscrambled experimental data.
The resulting test statistic is compared to the background only test statistic model to obtain the final p-value

No significant excess of neutrinos is seen in the test for a steady state flux scenario.
The best-fit test statistic is $\num{0.057}$ with best-fit parameters $\hat{n}_S = \num{5.57}$ and $\hat{\gamma} = \num{2.8}$.
This results in a p-value of $p=0.47$ and a significance of $\SIsigma{0.73}$.
The  best fit test statistic value together with the background only distribution can be seen in figure~(\ref{fig:time_int_bg_ts_best_fit}).
No post-trial correction needs to be applied.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics{plots/bg_ts_best_fit}
%   % \subimport*{plots/}{bg_ts_best_fit.pgf}
%   \caption{
%     Background only test statistic together with the experimental, unblinded result.
%     The best test statistic value of $\num{0.057}$ was shifted to twice its value to be visible in the plot.
%     In solid, the fitted $\chi^2$ PDF model which is used to estimate the p-value is overlaid.
%   }
%   \label{fig:time_int_bg_ts_best_fit}
% \end{figure}

