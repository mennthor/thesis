\chapter{Point source searches with IceCube neutrinos}
  \label{chp:pointsource}
In this chapter, the unbinned Likelihood methods used for IceCube point source searches are derived.
To identify an astrophysical neutrino signal from a specific source location on the sky, an excess of events from that direction needs to be identified.
In general, atmospheric and the sought after extraterrestrial neutrinos cannot be distinguished on a per event basis.
However, it is possible to search from a sample based point-of-view by measuring deviations for an ensemble of events with respect to a known background expectation.
A simple method could be to define a fixed search region around a direction from which signal is expected, count the number of measured events and compare them to the number of events from a background expectation in the region \cite{LiMa:1983ApJ}.
If the measured number of events is significantly higher than the expected number of background events that may be a hint for a signal from that direction.

In this thesis, a similar but more advanced form is used, incorporating multiple pieces of event information to increase the detection sensitivity.
Also the usage of pre-defined search regions, often called a binned approach, is replaced with an unbinned version on a per event basis.
This has the advantage of avoiding hard search region boundaries which can drop the sensitivity of the approach if e.g. an unknown source is located directly at the border of such a region.
Starting from a general unbinned, extended Likelihood approach, the special cases used in this thesis to carry out a time-dependent and time-integrated search for point-like sources are derived.
Particular cases handling multiple years of data from different detector configurations and multiple sources for the so-called stacking case are further derived from the basic form.
In the following, a single event is consistently noted with index $i$, a source with index $k$ and a data sample with index $j$.

\section{Extended unbinned Likelihood}
The extended Likelihood \cite{barlow1989statistics} and the corresponding logarithmic extended Likelihood function is defined as
\begin{equation}
  \mathcal{L}(\lambda) = \frac{\lambda^N e^{-\lambda}}{N!}\prod_{i=1}^N P_i
  \quad\Rightarrow\quad
    \ln\mathcal{L}(\lambda) = -\lambda+\sum_{i=1}^N \ln(\lambda P_i)
  \mcomma
\end{equation}
where the constant term $\ln(N!)$ is dropped in the logarithmic version.
Here $\lambda$ is the expected number of events and $N$ the number of measured events following a Poisson counting distribution.
The per event model distributions $P_i$, normalized to integral $1$ over the defined parameter space, describe the Likelihood of each event under the assumed model and how likely it contributes to the expectation.
The use of the Poisson term is justified by a re-normalization of the per event distributions to include the total number of measured events, which is not fixed for multiple experiments of the same kind but may fluctuate around an unknown expectation value.

The tested hypotheses are a priori encoded in the description of the model $P$.
To obtain a fairly general expression to derive the point source special cases from, the expectation model can be split into multiple classes by splitting the expectation and the models accordingly
\begin{equation}
  0 \leq \lambda = \sum_{k=1}^{N_\text{classes}} \lambda_k
  \mintertext{and}
  P_i = \frac{1}{\sum_{k=1}^{N_\text{classes}} \lambda_k}\cdot
         \sum_{k=1}^{N_\text{classes}} \lambda_k P_{i,k}
  \mperiod
\end{equation}
The single $\lambda_k$ can be negative but their sum must not, because it is still a Poisson expectation parameter.
Additionally, the new split model is normalized over all classes to arrive at the form
\begin{equation}
  \label{equ:extended_llh}
  \ln\mathcal{L}(\{\lambda_k\})
  = -\sum_{k=1}^{N_\text{classes}} \lambda_k +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{classes}}
      \lambda_k P_{i,k} \right)
  \mperiod
\end{equation}

To further specialize, it is usually desired to test a signal hypothesis against a background one, for $N_\text{srcs}$ sources in general, for each event $i$.
The above expression~(\ref{equ:extended_llh}) is thus expanded to include $N_\text{srcs}$ signal and $N_\text{srcs}$ background parameters and the corresponding distributions $S_{i,k}$ and $B_{i,k}$:
\begin{equation}
  \ln\mathcal{L}(\{\lambda_{k,S}\}, \{\lambda_{k,B}\})
  = -\sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,S}+\lambda_{k,B}\right) +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{srcs}}\left(
      \lambda_{k,S} S_{i,k}+\lambda_{k,B} B_{i,k}\right)\right)
  \mcomma
\end{equation}
and from the Poisson condition there is still the constraint
\begin{equation}
  0 \leq \sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,S}+\lambda_{k,B}\right)
  \mperiod
\end{equation}

For testing the significance of a potential signal contribution in the measured data, a Likelihood ratio test is used \footnote{See section~\ref{chp:pointsource_frequentist} for an introduction to frequentist methods.}.
The null hypotheses $H_0$, which means that only background is expected to be measured, is constructed by using only a portion $\Theta_0$ of the allowed parameter space, here by setting all signal expectations to zero
\begin{equation}
  \ln\mathcal{L}_0(\{\lambda_{k,S}=0\}, \{\lambda_{k,B}\})
  = -\sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,B}\right) +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{srcs}}\left(
      \lambda_{k,B} B_{i,k}\right)\right)
  \mperiod
\end{equation}
The alternative hypothesis $H_1$ is constructed by using the full Likelihood parameter space $\Theta$
\begin{equation}
  \ln\mathcal{L}_1(\{\lambda_{k,S}\}, \{\lambda_{k,B}\})
  = -\sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,S}+
                                     \lambda_{k,B}\right) +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{srcs}}\left(
      \lambda_{k,S} S_{i,k}+\lambda_{k,B} B_{i,k}\right)\right)
  \mperiod
\end{equation}

The Likelihood ratio test statistic $\Lambda$ for testing the null hypothesis $H_0$ against the alternative $H_1$ is defined as \cite{casella2002statistical}
\begin{equation}
  \ln\hat{\Lambda}
  = \ln\left(\frac{\sup_{\theta \in \Theta_0} \mathcal{L}(\theta)}
                  {\sup_{\theta \in \Theta} \mathcal{L}(\theta)}\right)
  = \ln\left(\sup_{\theta \in \Theta_0} \mathcal{L}(\theta)\right) -
    \ln\left(\sup_{\theta \in \Theta} \mathcal{L}(\theta)\right)
  \mperiod
\end{equation}
where $\hat{\Lambda}$ means the single test statistic value after finding the supremum of both nominator and denominator.
This leads to the test statistic considered in this work,
\begin{equation}
  \label{equ:extended_llh_ts}
  \begin{aligned}
    -2\ln\hat{\Lambda}
    &= 2\ln(\mathcal{L}_1(\{\hat{\lambda}_{k,S/B}\})) -
       2\ln(\mathcal{L}_0(\{\hat{\lambda}^{(0)}_{k,B}\})) \\
    &= -2\left(\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} +
                                         \hat{\lambda}_{k,B} -
                                         \hat{\lambda}_{k,B}^{(0)}\right) +
      2\sum_{i=1}^N \ln\left(
        \frac{\sum_{k=1}^{N_\text{srcs}}\left(
            \hat{\lambda}_{k,S} S_{i,k}+\hat{\lambda}_{k,B} B_{i,k}\right)}
            {\sum_{k=1}^{N_\text{srcs}}\left(
              \hat{\lambda}_{k,B}^{(0)} B_{i,k}
            \right)}
          \right)
    \mcomma
  \end{aligned}
\end{equation}
which has been decorated by the factor $-2$ to be compatible to Wilks' theorem \cite{Wilks:1938dza,casella2002statistical}.
Here the parameters $\hat{\lambda}_{k,S/B}$ were introduced, which mean the parameters $\lambda_{k,S/B}$ that maximize the Likelihood $\mathcal{L}_1$ under the complete parameter space and $\hat{\lambda}_{k,B}^{(0)}$ maximizing $\mathcal{L}_0$.
As seen in expression~(\ref{equ:extended_llh_ts}), all best-fit parameters from both hypotheses have to be distinguished.
That means it must be differentiated between the best fit parameters $\hat{\lambda}_{k,B}^{(0)}$ from the null hypothesis and $\hat{\lambda}_{k,B}$ from the alternative hypothesis, which are not the same in general.

In the following sections, specific model choices for the signal and background distributions and approximating assumptions are shown, to transform expression~(\ref{equ:extended_llh_ts}) to more commonly known forms, and to the ones used for the time-dependent and time-integrated search in this thesis.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %% General method to get the PDFs, not the used implementation
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Per event distributions}
The introduced per event distributions $S_i, B_i$ are the functions that actually define the tested hypothesis of the analysis.
Depending on their structure they can describe a single point source search, a stacking search for searching at various source positions at once or a template search, where a whole spatial region is tested for neutrino emission over the expectation.
The used PDFs are usually similar for all analysis types and the conventions applied for most point source searches in IceCube are followed here \cite{Braun:2010APh,Braun:2008bg}.

The per-event distributions describe the main separation power between signal and background hypotheses in combination with the mixing portions $\lambda_{i,S/B}$ by introducing a priori knowledge in defining signal- and background-like regions in the tested parameter space.
The better these distributions are able to separate signal and background regions, the more sensitive the analysis becomes, but also the more bias towards specific model choice is introduced.
A common approach with known good separation power is to combine contributions from spatial clustering and energy information, where the first one is inherent to the tested point source hypothesis and the latter providing additional information under certain assumptions of signal flux shapes.
For time-dependent analyses, an additional time-dependent part is introduced.

In their general form, the model PDFs $S_i$ and $B_i$ are multi-dimensional composite PDFs.
For the common approach using spatial, energy and time information, the resulting PDFs would have five dimensions, being possibly correlated in the variables declination, right-ascension, energy proxy, the spatial uncertainty estimator and the event times.
However, for most IceCube point source searches, the signal and background contributions are written as independent products of a spatial, an energy and a time-dependent part as
\begin{align}
  \label{equ:general_pdfs}
  S_{i,k}
    &= S(\vec{x}_i, \vec{x}_{\mathrm{src},k}, E_i | \gamma)
     = S^S(\vec{x}_i, \vec{x}_{\mathrm{src},k}) \cdot
       S^E(E_i, \delta_i | \gamma) \cdot
       S^T(t_i, t_k) \\
  \intertext{and}
  B_{i,k}
    &= B(\delta_i, E_i | \phi_\mathrm{BG})
     = B^S(\delta_i) \cdot
       B^E(E_i, \delta_i | \phi_\mathrm{BG}) \cdot
       B^T(t_i, t_k)
\end{align}
where $\gamma$ is the shape parameter of a signal flux usually assumed to be a power law $\propto E^{-\gamma}$ and $\phi_\mathrm{BG}$ stands for a flux model of the atmospheric neutrino flux describing the background flux dependency.
The event times $t_i$ and source times $t_k$ define the time dependent emission-model and $\delta_i$ is the declination angle, described in more detail in chapter~\ref{chp:datasets}.
The choice of splitting the PDF in several independent terms happens for symmetry and ease of use reasons.
The first step to arrive at the split PDF form, is to apply the law of total probability \cite{casella2002statistical}, which dissects the composite PDF into marginal distributions
\begin{equation}
  f(x_1, \dots, x_n)
  = P(x_1) \cdot P(x_2 | x_1) \cdot P(x_3 | x_1 \and x_2) \dots
    P(x_n | x_1 \and \dots \and x_{n-1})
  \mperiod
\end{equation}
This splits the combined PDF into products of marginal distributions.
By neglecting the marginal dependency e.g. of the split off energy PDF on the spatial resolution proxy or the energy dependence on the spatial terms, the standalone PDFs in the following sections can be derived.
Because of the special location of IceCube, often the right-ascension dependency is dropped, because of symmetry arguments.
Also, for the spatial PDF, it is chosen to replace the general form with an analytic Gaussian PDF as it resembles the true distribution closely enough but is much easier to handle.
Note, that all these simplifications and assumptions influence the shape of the obtained test statistics and may lead to deviations of the expected shape from the application of Wilks' theorem.
Also note, that only variables known on experimental data can be used and no simulation truth is known on actual data.

\subsection{Spatial distribution}
  \label{chp:pointsource_signal_pdf}
The most important part in the search for neutrino point sources is the spatial clustering of events around a point in the sky.
Without the spatial term, only an insensitive diffuse analysis would be performed.
For data samples used in point source searches, a reconstructed estimate of the per event uncertainty is available.
This estimator is built from the positional reconstruction Likelihood fit and is constructed assuming a symmetric, two-dimensional Gaussian distribution describing the reconstruction uncertainty.
The value is obtained by scanning the reconstruction Likelihood on a grid and fitting a two-dimensional parabola through the sampled points to be compliant with the assumed underlying Gaussian distribution.
To be even easier to apply, it is further assumed that the underlying distribution is symmetric in the two directional angles.
To obtain the symmetrized covariance matrix from the generally tilted form, the two-dimensional covariance matrix
\begin{equation}
  \begin{pmatrix}
    \sigma_a^2 & \sigma_b^2 \\
    \sigma_b^2 & \sigma_c^2 \\
  \end{pmatrix}
\end{equation}
can be transformed in its diagonal form.
The new covariance ellipsis axes then have the values
\begin{equation}
  \sigma_{1,2}^2 = \frac{\sigma_a^2+\sigma_b^2}{2} \pm
    \sqrt{(\sigma_a^2 + \sigma_b^2)^2 + 4\sigma_c^2}
  \mperiod
\end{equation}
Demanding that the half-axes are of equal length, the radicand needs to vanish, leaving the relation
\begin{equation}
  \sigma_1^2 = \sigma_2^2 = \frac{\sigma_a^2+\sigma_b^2}{2}
  \mcomma
\end{equation}
which is used to circularize the covariances obtained from the grid scan.
Because the Gaussian is thus of circular shape, only the radial distance, or space angle, from the assumed source position the each event's position is needed
\begin{equation}
  \cos(\Psi_{i,k})
  = \vec{x}_k \cdot \vec{x}_i
  = \cos(\alpha_i - \alpha_k) \cdot
    \cos(\delta_i)\cos(\delta_k) +
    \sin(\delta_i)\sin(\delta_k)
  \mcomma
\end{equation}
which can be derived from the scalar product in spherical coordinates with the angles defined by the equatorial coordinate system convention.

Alternatively, the same value can be transformed and used for the symmetric Kent distribution \cite{Kent:1982pdf} for the per event spatial distribution instead.
The Kent distribution is the pendant of the symmetric, two-dimensional Gaussian distribution, but is correctly normalized on the sphere and is useful for larger uncertainties for example when investigating cascade-like events.
For tracks, the good angular resolution of about \SI{1}{\degree} justifies the use of the simpler and more familiar Gaussian distribution as both distributions become virtually indistinguishable for small uncertainties.

In general, the joint probability for an event $i$ to spatially originate from a source $k$ can be obtained by a convolution of the two separate spatial PDFs.
One part describes the intrinsic distribution for the source and the other part the intrinsic distribution of the event itself.
In a search for point sources at known positions, the distribution of a source position is represented by a delta distribution, in searches for spatially extended sources usually, a Gaussian profile is assumed.
The per-event probability distribution is assumed to be a Gaussian PDF, which is connected to the construction of the per-event-uncertainty from a parabolic fit of the scanned Likelihood space, as described above or a bootstrap algorithm.
The resulting spatial signal distribution, describing the probability of an event being spatially correlated to a given source with density profile $f(\vec{x}_k)$, can thus be expressed as a convolution of the Gaussian per event part and source distribution for the fixed source position
\begin{equation}
  \label{equ:spatial_pdf}
  S^S(\vec{x}_i, \vec{x}_k)
  = \int_\Omega \frac{1}{2\pi \sigma_i^2}
    \exp\left(-\frac{|\vec{x}-\vec{x}_i|^2}{2\sigma_i^2}\right) \cdot
    f(\vec{x}_k - \vec{x}) \d{\vec{x}} \\
    \mperiod
\end{equation}
For the point-source hypothesis, $f(\vec{x}_k)$ can be replaced with a delta distribution $\delta(\vec{x}_k)$ yielding the often used expression
\begin{equation}
  S^S(\vec{x}_i, \vec{x}_k)
  = \frac{1}{2\pi \sigma_i^2}
    \exp\left(-\frac{|\vec{x}_k-\vec{x}_i|^2}
                    {2\sigma_i^2}\right)
\end{equation}
for the spatial signal PDF.
Assuming extended sources with a Gaussian density profile would follow the same scheme and an analytic solution for the convolution of two Gaussian distributions exists, resulting in a substitution of $\sigma_i \rightarrow \sqrt{\sigma_i^2 + \sigma_k^2}$ \cite{Bromiley2014ProductsAC}.
Note, that the energy dependence of the signal PDF is implicitly included in the per-event uncertainties marginal distribution.
The marginal term, that needs to be split-off in the application of the law of total probability, resembles the energy distribution, which is then dealt with separately.

The spatial background distribution is constructed similarly to the signal case in equation~(\ref{equ:spatial_pdf}).
Because IceCube is a nearly right-ascension symmetric detector at the south pole, the distribution is assumed to be declination dependent only, which holds the better the larger the regarded time windows are.
This may break down, however, for time scales short enough that the earth's rotation does not smear out the slightly asymmetric distribution in azimuth angle.
The declination dependent PDF is written in a general form here and can, for example, be constructed by computing histograms of experimental data, which is described later in more detail.
The spatial background distribution can then be written as
\begin{equation}
  B^S(\delta_i) = \frac{1}{2\pi} \cdot P(\sin(\delta_i))
  \mcomma
\end{equation}
where the first factor is the uniform distribution in right-ascension and the latter indicating, the often alternatively used, dependence on the sine of declination $\delta$.
Note that the background distribution is only depending on the event position, as background  events are not expected to have any correlation with a source, by definition.
This description neglects the per-event uncertainty though.
In principle, the same method that was used to construct the convoluted signal PDF can be applied to the construction of the background distribution.
With
\begin{equation}
  B^S(\vec{x}_i) =
    \int_\Omega \hat{B}^S(\vec{x}) \cdot
      \exp\left(-\frac{|\vec{x}-\vec{x}_i|^2}{2\sigma_i^2}\right)
      \,\d{\vec{x}}
\end{equation}
the per-event declination distribution would be described in full detail.
However, as the intrinsic background PDF $\hat{B}^S$ does not change much with declination and a closed form analytic description as in the Gaussian case for the signal PDF in~(\ref{equ:spatial_pdf}) is not easily possible with $\hat{B}^S$ derived from histograms, the per-event-uncertainty is neglected.


\subsection{Energy distribution}
In addition to the spatial clustering, the event's energy can also provide a powerful separation argument and lead to a large improvement in sensitivity \cite{Braun:2008bg}.
As the energy distribution of atmospheric neutrinos can approximately be described by a power law $\phi_\mathrm{BG}(E) \propto E^{-3.7}$ and the astrophysical signal by a harder spectrum around $\phi(E)_S \propto E^{-2}$, higher energy events are more likely to originate from an extraterrestrial source rather than having been created in the atmosphere.
In the PDFs, the energy dependence can only be taken into account with energy estimators of the true neutrino energy $E_\nu$ because the information must also be available on measured data, not knowing the true neutrino energy.
Formally, that mapping can be written as an integration using the law of total probabilities of the distribution of the energy proxy of the event with the probability of obtaining a true neutrino energy under the current flux hypothesis
\begin{equation}
  \hat{S}^E(E, \vec{x}|\gamma) =
    \int_{E_\nu}\int_{\Omega_\nu}
    P(E, \vec{x}|E_\nu, \vec{x}_\nu) \cdot P(\vec{x}_\nu, E_\nu|\gamma)
    \,\d{E_\nu}d{\vec{x}_\nu}
    \mcomma
\end{equation}
where $\gamma$ is the shape parameter of an assumed signal power law flux.

For background, the same reasoning applies and the flux model is substituted for one describing the atmospheric neutrino background instead
\begin{equation}
  \hat{B}^E(E, \vec{x}|\phi_\mathrm{BG}) =
    \int_{E_\nu}\int_{\Omega_\nu}
    P(E, \vec{x}|E_\nu, \vec{x}_\nu) \cdot P(\vec{x}_\nu, E_\nu|\phi_\mathrm{BG})
    \,\d{E_\nu}d{\vec{x}_\nu}
\end{equation}
to obtain the intrinsic distribution.

In practice, the integrals may be obtained using histograms in reconstructed declination and an energy estimator from simulations for signal and from either simulations or measured data for the background distributions.
By computing the histograms on the proxy variables directly, the integrals are obtained automatically in each bin from the simulation procedure.
This is because each bin contains the superposition of the fraction of true neutrino energies contributing to the proxy value considered in the bin.

To obtain the convolved PDFs, that also take into account the per event uncertainty for each event in energy and position, the same ansatz as for the signal PDFs in section~\ref{chp:pointsource_signal_pdf} can be used
\begin{align}
  S^E(E_i, \delta_i|\gamma) &=
    \int_E\int_\Omega f(\vec{x}_i, E_i)\hat{S}^E\,\d{E}\d{\vec{x}} \\
  B^E(E_i, \delta_i|\phi_\mathrm{BG}) &=
    \int_E\int_\Omega P(E_i,\delta_i|E_\nu)\cdot P(E_\nu|\phi_\mathrm{BG})
      \d{E_\nu}
    \mperiod
\end{align}
In the majority of analysis, a delta-distribution is used for the per-event-function in the last step, for the same reasons quoted for the spatial background PDF.
Also, the distribution is normalized independently of the spatial PDFs, although the declination occurs in both of them.
This may be done to simplify the computation and housekeeping in analysis code because it avoids higher dimensional PDF constructions.

\subsection{Time dependency}
When testing for time dependent emission models, time dependent PDFs $S_{i,k}^T(t_i, t_k)$ and $B_{i,k}^T(t_i, t_k)$ need to be incorporated, which depend on each event's time and their occurrences relative to the temporal evolution of the sources.
The assumption is then, that each source only emits neutrinos as given by $S_{i,k}^T(t_i, t_k)$.
The background can in general also be time dependent to account e.g. for seasonal variations in the detector rate.
Because of the precise timing information available per event, there is no relevant time uncertainty that needs to be convolved into the intrinsic PDFs.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %% Special case GRB LLH
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Time dependent Likelihood}
  \label{chp:pointsource_tdep_llh}
To test for a time-dependent emission in this analysis, the general extended Likelihood is altered to a form similar to what is usually called \emph{Gamma Ray Burst Likelihood}\footnote{Named after the original purpose of searching for emission from a Gamma Ray Burst catalogue. The concept can be applied to other sources testing a similar hypothesis of emission within a defined time window.} \cite{Aartsen:2017zvw,Aartsen:2014aqy,Aartsen:2017wea}.
As the name may indicate, the source emission is assumed to be per-burst with a fluence $\Phi$ rather than a time-dependent flux, where fluence means the integral form of a time-dependent flux, so particles received per energy and unit area only.

This includes explicit assumptions about the temporal source emission PDFs and simplifications of the general Likelihood formula introduced before.
The simplifications need to be applied to ensure the proper handling of small time windows.
For relatively small burst time windows, only a few events are left in each window making it hard to fit a larger amount of free model parameters to the data.
Therefore the number of free parameters needs to be reduced as much as possible without introducing too many a priori assumptions.
That allows to stick to a rather general search method because of the unknown source types.

Here, using the burst model, a rather general assumption about the source emission is made.
The time PDFs are therefore constructed using rectangle functions
\begin{equation}
  \rect(t)
  = \Theta\left(t+\frac{1}{2}\right) \cdot \Theta\left(t-\frac{1}{2}\right)
  = \begin{cases}
      1 &\text{ , if}\quad |t| \leq \frac{1}{2} \\
      0 &\text{ , if}\quad  t    >  \frac{1}{2} \\
    \end{cases}
  \mcomma
\end{equation}
where $\Theta$ is the Heaviside step function \cite{Abramowitz:1974handbook}.
This way, each source is having only a non-zero emission contribution within its pre-defined source time window
\begin{equation}
  \label{equ:time_window_def}
  S_{i,k}^T = B_{i,k}^T = T_k(t_i) \coloneqq
    \rect\left(\frac{t_i - \frac{t_k^1-t_k^0}{2}}
                              {t_k^1-t_k^0}\right)
  \mperiod
\end{equation}
This effectively cuts out a subset of events around the sources time stamps $t_k$ and the corresponding time interval around each source $[t_k^0, t_k^1]$ and is a background rejection technique only, because any underlying temporal dependence on the source emission flux is discarded.
The most simple case is then to have each source in its own, non-overlapping time window so that each source has a unique set of events belonging to it.
Note again, that no separation-power stems from these time PDFs, they are merely used to reduce the background rate under the assumption of a temporally concentrated emission.
In this light, a per-burst search is not a \enquote{real} time-dependent search.
For the ease of use, a time PDF is only defined to effectively cut out the interesting burst regions.

One important simplification of the general Likelihood applied here, is that the background expectations are not fitted, but rather fixed from the integrated off-time data rate over the range of the background time PDFs.
This decreases the number of parameters to fit for because it unifies and fixes the background estimators to
\begin{equation}
  \hat{\lambda}_{k,B} = \hat{\lambda}_{k,B}^{(0)} = \Braket{\lambda_{k,B}}
  \mperiod
\end{equation}
The test statistic then turns to the form
\begin{equation}
  -2\ln\hat{\Lambda}
  = -2\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} +
      2\sum_{i=1}^N \ln\left(
        \frac{\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} S_{i,k}}
             {\sum_{k=1}^{N_\text{srcs}}\Braket{\lambda_{k,B}} B_{i,k}}
        + 1
      \right)
  \mperiod
\end{equation}

One further Likelihood simplification is performed, which is necessary because a large number of free parameters cannot be fitted to very few events in a low background analysis.
The relative signal expectations of each source are fixed a priori and the Likelihood is only fitted for a total signal expectation value
\begin{equation}
  \label{equ:ns_split_weights_per_src}
  \lambda_{k,S} = n_S \cdot w_k
\end{equation}
with a free global signal strength parameter $n_S$ and weights $w_k$ normalized so that
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}} w_k = 1
  \mperiod
\end{equation}
The test statistic can then be expressed by
\begin{equation}
  \label{equ:tdep_ts_general}
  -2\ln\hat{\Lambda}
  = -2\hat{n}_S +
      2\sum_{i=1}^N \ln\left(
        \frac{\hat{n}_S \sum_{k=1}^{N_\text{srcs}} w_k S_{i,k}}
             {\sum_{k=1}^{N_\text{srcs}}\Braket{\lambda_{k,B}} B_{i,k}}
        + 1
      \right)
  \mperiod
\end{equation}

The a priori fixed weights resemble the expectation from every single source at the detector.
They can, therefore, depend on the explicitly chosen source emission model and the detection efficiency depending on the source locations in the detector.
Note that the a priori chosen weights should match the true, but usually unknown, emission scenario as much as possible to obtain a good analysis sensitivity.
If the true scenario strongly differs from the assumed weights, the performance of the analysis drops.
Generalized assumptions can be made though, in favour of the ability to test a broader range of emission scenarios, but loosing performance when nature indeed chose a specific model realisation.


\subsection{Single sample stacking case}
  \label{chp:pointsource_tdep_llh_single}
Using the assumption of independent time windows for the sources can further simplify the test statistic.
The expression can be rearranged to an explicit sum of logarithms that better resembles the uniqueness of each source in its time window.

Starting from the test statistic expression~(\ref{equ:tdep_ts_general})
% \begin{equation}
%   -2\ln\hat{\Lambda}
%   = -2\hat{n}_S +
%       2\sum_{i=1}^N \ln\left(
%         \frac{\hat{n}_S \sum_{k=1}^{N_\text{srcs}} w_k S_{i,k}}
%              {\sum_{k=1}^{N_\text{srcs}}\Braket{\lambda_{k,B}} B_{i,k}}
%         + 1
%       \right)
%   \mcomma
% \end{equation}
and from the definition of the unique time windows, it can be seen that each event $i$ can only contribute to a single source $k$.
The event belongs to the source in which time window it falls or to no source at all, where the time windows are defined as in~(\ref{equ:time_window_def})
\begin{equation}
  T_k(t_i) \coloneqq \rect \left(
    \frac{t_i - \frac{t_k^1-t_k^0}{2}} {t_k^1-t_k^0}
  \right)
  \mperiod
\end{equation}
The stacking sum then turns to
\begin{align}
  -2\ln\hat{\Lambda}
  &= -2\hat{n}_S +
      2\sum_{i=1}^N \ln\left(
        \frac{\hat{n}_S \sum_{k=1}^{N_\text{srcs}} w_k S_{i,k}
              \delta_{\{i,k|T_k(t_i)\neq 0\}}}
             {\sum_{k=1}^{N_\text{srcs}}\Braket{\lambda_{k,B}} B_{i,k}
              \delta_{\{i,k|T_k(t_i)\neq 0\}}}
        + 1
      \right) \\
  &= -2\hat{n}_S +
      2\sum_{i=1}^N \ln\left(
        \frac{\hat{n}_S \left[0+\dots+0+ w_{k^*} S_{i,k^*} +0+\dots+0\right]}
             {\left[
              0+\dots+0+ \Braket{\lambda_{k^*,B}} B_{i,k^*} +0+\dots+0
              \right]}
        + 1
      \right) \\
  &= -2\hat{n}_S +
      2\sum_{i=1}^N \sum_{k=1}^{N_\text{srcs}} \ln\left(
        \frac{\hat{n}_S w_k S_{i,k}}{\Braket{\lambda_{k,B}} B_{i,k}}
        + 1
      \right)
  \mcomma
\end{align}
where $k^*$ means the $k$ that fulfils the condition $T_k(t_i)\neq 0$ and in the last step it is used that
\begin{equation}
  \ln\left(
      \frac{\hat{n}_S w_{k\neq k^*} S_{i,k\neq k^*}}
           {\Braket{\lambda_{k\neq k^*,B}} B_{i,k\neq k^*}}
      + 1
    \right)
    = \ln(0 + 1) = 0
  \mperiod
\end{equation}

\subsection{Multiple samples stacking case}
To add more sources to the unique time window scenario, data at the times at which the added source events occurred needs to be taken into account.
In each IceCube data taking season, there may be a change in the data taking procedure, for example in the trigger system.
Also, different sample selection methods may yield different expectation values for the number of events in each sample.
These changes and differences must be included in the expectations for the samples used in the single Likelihood test.
An additional weighting scheme can be considered to account for the differences.
The reasoning is quite similar to the a priori $n_S$ splitting weights $w_k$ for the stacking case for the single sample Likelihood as seen in~(\ref{equ:ns_split_weights_per_src}).

To add another sample, the individual logarithmic Likelihoods can be summed up, because the tested datasets are independent, which yields the combined test statistic
\begin{align}
  -2\ln\hat{\Lambda}
  &= \sum_{j=1}^{N_\text{sam}} -2\ln\hat{\Lambda}_j(\hat{n}_{S,j}) \\
  &= \sum_{j=1}^{N_\text{sam}} \left[
        -2\hat{n}_{S,j} +
        2\sum_{i=1}^{N_j} \sum_{k=1}^{N_\text{srcs,j}} \ln\left(
          \frac{\hat{n}_{S,j} w_{k,j} S_{i,k,j}}
               {\Braket{\lambda_{k,j,B}} B_{i,k,j}}
          + 1
        \right)
      \right]
  \mcomma
\end{align}
where individual free $n_{S,j}$ signal parameters are introduced and the stacking weights $w_{k,j}$ per source in the sample are constructed like the usual single sample stacking weights $w_k$ and are still normalized over all sources $N_\text{srcs}$ per sample
\begin{equation}
  \sum_{k=1}^{N_{j,\text{srcs}}} w_{k,j} = 1
  \mperiod
\end{equation}
Only the number of events per sample $N_j$ and the sources belonging to sample $j$ are considered for each Likelihood, as well as the weights $w_{k,j}$, PDFs $S_{i,k,j}$ and $B_{i,k,j}$ and the background expectations per source $\Braket{\lambda_{k,j,B}}$ for each sample.
Again, a priori information about the expected number of signal events originating from each sample can be used and a global free signal strength parameter $n_S$ is introduced with
\begin{equation}
  n_{S,j} = w_j n_S
  \mperiod
\end{equation}
To calculate the a-priori weights $w_j$ the law of total probability is applied
\begin{equation}
  w_j = P(j) = \sum_{k=1}^{N_\text{srcs}} P(j|k)\cdot P(k)
  = \sum_{k=1}^{N_\text{srcs}} \underbrace{P(j,k)}_\text{unknown}
  \mcomma
\end{equation}
because only the conditional probability contribution $P(j|k)$ from each source $k$ per sample $j$ is known, but not the joint distribution $P(j,k)$.
% That means, the needed probability $P(j)$ of getting a signal contribution $n_S w_j$ from sample $j$ splits into $P(j|k)$ and $P(k)$.
$P(j|k)$ is the probability of getting signal from source $k$ within sample $j$, normalized over all samples
\begin{equation}
  \sum_{j=1}^{N_\text{sam}} P(j|k) = 1
  \mperiod
\end{equation}
Additionally, $P(k)$ is the probability of getting signal from source $k$ at all within any sample, separately normalized over all sources
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}} P(k) = 1
  \mperiod
\end{equation}
These relations can also be written in a concise matrix notation
\begin{equation}
  \label{equ:sample_w_matrix}
  \begin{pmatrix} w_1 \\ \vdots \\ w_{N_\text{sam}} \end{pmatrix} =
    \begin{pmatrix}
      P(j=0|k=0) & \dots & P(j=0|k=N_\text{srcs}) \\
      \vdots & \ddots & \vdots \\
      P(j=N_\text{sam}|k=0) & \dots & P(j=N_\text{sam}|k=N_\text{srcs})
    \end{pmatrix} \cdot
    \begin{pmatrix}
      P(k=0) \\ \vdots \\ P(k=N_\text{srcs})
    \end{pmatrix}
  \mperiod
\end{equation}
The unnormalized, conditional signal expectation values can be obtained in each sample by calculating the expected number of events from a signal simulation which usually differs for each detector configuration and sample selection criteria.
These values can then be used to normalize the matrix per column and construct the $P(k)$ vector by summing over each column per source $k$.

The most complex weighting case in this scenario would be having multiple sources that have time PDFs overlapping in their emission region and are also leaking into another data sample.
The formalism then still applies, and the sample splitting weights can be obtained by integrating the time emission PDFs per sample to obtain the relative emission strength for the source portions lying in each sample.
These are then multiplied with the usual declination dependent weights per sample to form the $P(j|k)$ entries of the matrix in expression~(\ref{equ:sample_w_matrix}).
The weighting within each sample wouldn't be affected and is still constructed as explained in the previous section~\ref{chp:pointsource_tdep_llh_single}.

For the special case treated here, with each source having its unique time window and also falling exclusively in a single sample, each column has only a single entry which is $1$ after the trivial normalization.
The probabilities $P(k)$ can be obtained by using the un-normalized source stacking weights regardless of the sample $\tilde{w}_k$ and re-normalize them over all sources in all samples
\begin{equation}
  P(k) = \frac{\tilde{w}_k}{\sum_{m=1}^{N_\text{srcs}} \tilde{w}_m}
  \mperiod
\end{equation}
Because of the special matrix properties used here, the explicit weights $w_j$ then turn out to be global re-normalizations of the un-normalized single sample weights $\tilde{w}_k$ with
\begin{align}
  w_j
    &= \sum_{k=1}^{N_\text{srcs}} P(j|k)\cdot P(k)
    = \sum_{k=1}^{N_\text{srcs}}
      \delta_{\{k,j|T_j(t_k)\neq 0\}} \cdot
      \frac{\tilde{w}_k}{\sum_{m=1}^{N_\text{srcs}} \tilde{w}_m} \\
    &= \sum_{k=1}^{N_{j,\text{srcs}}}
      \frac{\tilde{w}_{k,j}}{\sum_{m=1}^{N_\text{srcs}} \tilde{w}_m}
  \mcomma
\end{align}
where $T_j$ is a unique rectangle function for each sample, equally used as the rectangle function utilized to describe the unique time windows per source in each sample.
$\tilde{w}_{k,j}$ are the un-normalized stacking weights per sample.

Now the numerator turns out to be exactly the per sample normalization of the per sample splitting weights, which is the sum of all weights for the subset of all sources that actually are in the sample.
The full multi-sample test statistic then reads
\begin{equation}
  -2\ln\hat{\Lambda}
  = -2\hat{n}_S +
      2\sum_{j=1}^{N_\text{sam}} \sum_{i=1}^{N_j} \sum_{k=1}^{N_\text{srcs,j}}
      \ln\left(
        \frac{\hat{n}_{S}\frac{\tilde{w}_{k,j}}{\sum_{m=1}^{N_\text{srcs}}
              \tilde{w}_m} S_{i,k,j}}
             {\Braket{\lambda_{k,j,B}} B_{i,k,j}}
        + 1
      \right)
  \mcomma
\end{equation}
where $\tilde{w}_{k,j}$ are the un-normalized weights per source with respect to the expected signal in their corresponding sample and are normalized over all un-normalized source expectations $\tilde{w}_m$ in all samples regarded.
This expression nicely demonstrates the circumstances here, namely that each source is independent of each other source and lies completely in a single data sample, so the whole underlying Likelihood completely factorizes in events, sources and samples.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %% Derivation of classic time integrated LLH used within Skylab
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Time integrated Likelihood}
  \label{chp:pointsource_time_int_llh}
The standard time integrated Likelihood formula used in IceCube point source searches can also be derived from the general, extended Likelihood formula.
A different approximation than in the time-dependent case is used, which takes into account the usually larger statistics in a time-integrated sample as all events count and not only these in temporal coincidence with any source.

\subsection{Single sample stacking case}
Starting again from the general form
\begin{equation}
  -2\ln\hat{\Lambda}
  = -2\left(\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} +
                                      \hat{\lambda}_{k,B} -
                                      \hat{\lambda}_{k,B}^{(0)}\right) +
    2\sum_{i=1}^N \ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}\left(
          \hat{\lambda}_{k,S} S_{i,k}+\hat{\lambda}_{k,B} B_{i,k}\right)}
          {\sum_{k=1}^{N_\text{srcs}}\left(
            \hat{\lambda}_{k,B}^{(0)} B_{i,k}
          \right)}
        \right)
  \mcomma
\end{equation}
the following approximation can be used
\begin{equation}
  \label{equ:time_indep_fixed_expectation}
  \sum_{k=1}^{N_\text{srcs}} \hat{\lambda}_{k,S} + \hat{\lambda}_{k,B} \approx
    \sum_{k=1}^{N_\text{srcs}} \hat{\lambda}_{k,B}^{(0)} \approx N
  \mperiod
\end{equation}
This means the Poisson fluctuations of the sample size are neglected.
Also, the second part is usually valid if the amount of signal expected in the data is small compared to the amount of background-like events.

These approximations cancel the Poisson term in front of the sum and leave
\begin{equation}
  -2\ln\hat{\Lambda}
  = 2\sum_{i=1}^N \ln\left(
    \frac{\sum_{k=1}^{N_\text{srcs}}\left(
          \hat{\lambda}_{k,S} S_{i,k}+\hat{\lambda}_{k,B} B_{i,k}\right)}
          {\sum_{k=1}^{N_\text{srcs}}\left(
            \hat{\lambda}_{k,B}^{(0)} B_{i,k} \right)}
        \right)
  \mperiod
\end{equation}
For the time-independent part, the background distributions $B_{i,k}$ are all the same, because they only depend on each event's location and not on any source related parameters anymore.
Thus the background PDFs can be written as
\begin{equation}
  \hat{\lambda}_{k,B} = \frac{1}{N_\text{srcs}} \hat{\lambda}_B
  \mintertext{and} B_{i,k} = B_i
\end{equation}
and with approximation~(\ref{equ:time_indep_fixed_expectation}) the denominator in the logarithm can be simplified to
\begin{equation}
  \frac{1}{\sum_{k=1}^{N_\text{srcs}}\left(
           \hat{\lambda}_{k,B}^{(0)}B_{i,k} \right)}
  = \frac{1}{B_i \sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,B}^{(0)}}
  = \frac{1}{N B_i}
  \mperiod
\end{equation}
Using the same argument for the background distributions in the nominator, the stacking test statistic becomes
\begin{align}
  -2\ln\hat{\Lambda}
  &= 2\sum_{i=1}^N \ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}
            \left(
              \hat{\lambda}_{k,S} S_{i,k} +
              \frac{1}{N_\text{srcs}} \hat{\lambda}_B
            \right)}{N B_i}
     \right) \\
  &= 2\sum_{i=1}^N \ln\left(
    \frac{\sum_{k=1}^{N_\text{srcs}}\left(\hat{\lambda}_{k,S} S_{i,k}\right) +
          \hat{\lambda}_B B_i}{N B_i} \right)
  \mperiod
\end{align}

Using the fixed expectation approximation from~(\ref{equ:time_indep_fixed_expectation}) again, the background parameter $\hat{\lambda}_B$ can be eliminated leaving $N_\text{srcs}$ free signal parameters, one for each source
\begin{align}
  -2\ln\hat{\Lambda}
  &= 2\sum_{i=1}^N \ln\left(
    \frac{\sum_{k=1}^{N_\text{srcs}}\left(\hat{\lambda}_{k,S} S_{i,k}\right) +
          \left(
            N - \sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S}
          \right) B_i}{N B_i} \right) \\
  &= 2\sum_{i=1}^N \ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}\left(\hat{\lambda}_{k,S}
              S_{i,k}\right)}{N B_i} -
      \frac{\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S}}{N} + 1
    \right)
  \mperiod
\end{align}
where in the last step the equation is slightly rearranged to show the commonly used test statistic formula in the general time integrated stacking case.

The $N_\text{srcs}$ free signal strength parameters $\lambda_{k,S}$ can again be reduced to a single signal strength parameter $n_S$ when a priori knowledge about the source class proportions is used via
\begin{equation}
  \lambda_{k,S} = n_S \cdot w_k
  \mintertext{and}
  \sum_{k=1}^{N_\text{srcs}} w_k = 1
  \mperiod
\end{equation}
The test statistic can then be further reduced to its final form
\begin{align}
  -2\ln\hat{\Lambda}
  &= 2\sum_{i=1}^N \ln\left(
        \frac{\hat{n}_S \sum_{k=1}^{N_\text{srcs}}\left(w_k S_{i,k}\right)}
             {N B_i} -
        \frac{\hat{n}_S \sum_{k=1}^{N_\text{srcs}}w_k}{N} + 1
      \right) \\
  &= 2\sum_{i=1}^N \ln\left(
        \frac{\hat{n}_S}{N}\left(
          \frac{\sum_{k=1}^{N_\text{srcs}}(w_k S_{i,k})}{B_i} - 1
        \right) + 1
      \right)
  \mperiod
\end{align}
Sometimes the signal sum term is abbreviated to
\begin{equation}
  S_i^\text{(tot)} \coloneqq \sum_{k=1}^{N_\text{srcs}}\left(w_k S_{i,k}\right)
\end{equation}
and only in this case, with a-priori fixed weights $w_k$, the replacement of the single source signal term $S_i$ with the summed signal term $S_i^\text{(tot)}$ is valid
\begin{equation}
  \sum_{i=1}^N \ln\left(
      \frac{\hat{n}_S}{N}\left( \frac{S_i}{B_i} - 1 \right) + 1
    \right)
  \rightarrow
  \sum_{i=1}^N \ln\left(
      \frac{\hat{n}_S}{N}\left( \frac{S_i^\text{(tot)}}{B_i} - 1 \right) + 1
    \right)
\end{equation}
to quickly change from a single to a stacked source hypothesis.

\subsection{Multiple samples stacking case}
Construction of the multi-sample Likelihood formula in the time-integrated case is done exactly as in the time-dependent case before by starting with the sample weight relation
\begin{equation}
  n_{S,j} = n_S \cdot w_j
  \mintertext{with}
  w_j = P(j) = \sum_{k=1}^{N_\text{srcs}} P(j|k)\cdot P(k)
\end{equation}
inserted in the familiar combination of single sample Likelihoods
\begin{align}
  -2\ln\hat{\Lambda}
  &= \sum_{j=1}^{N_\text{sam}} -2\ln\hat{\Lambda}_j(\hat{n}_{S,j}) \\
  &= \sum_{j=1}^{N_\text{sam}} \left[
      2\sum_{i=1}^{N_j} \ln\left(
        \frac{\hat{n}_{S,j}}{N_j}\left(
          \frac{\sum_{k=1}^{N_\text{srcs}}(w_{k,j} S_{i,k,j})}{B_{i,j}} - 1
        \right) + 1
      \right)
    \right] \\
  &= \sum_{j=1}^{N_\text{sam}} \left[
      2\sum_{i=1}^{N_j} \ln\left(
        \frac{\hat{n}_S \cdot w_j}{N_j}\left(
          \frac{\sum_{k=1}^{N_\text{srcs}}(w_{k,j} S_{i,k,j})}{B_{i,j}} - 1
        \right) + 1
      \right)
    \right]
  \mperiod
\end{align}
The number of events $N_j$ is now related to the number of events per sample $j$ and the number of sources is the same in each sample, because of the steady state emission scenario.
Also the PDFs $S_{i,k,j}$, $B_{i,j}$ and stacking weights $w_{k,j}$ are taken and normalized per sample as seen before in the time dependent Likelihood in section~\ref{chp:pointsource_tdep_llh}.

Because all sources contribute during all times and thus in all samples, the weights cannot be written out in a simplified, compact form here, but they still represent the relative sensitivity of a single sample to a given source hypothesis combined with the global sensitivity to a single source across all samples.
This may also be seen as a special case for the time-dependent Likelihood weights, where all the time windows for signal and background are as large as the sample livetimes and a true flux scenario is used, where the emitted total flux is proportional to the emission time window length.

A notable difference arises if another global fit parameter is introduced alongside the mandatory $n_S$.
This is often done to better adapt to the unknown signal hypothesis and in a time-integrated analysis, the statistics are usually sufficient to reliably fit an additional parameter.
Usually, this parameter describes the signal flux hypothesis and is modelled as a single unbroken power law with spectral index $\gamma_k$ per source in the energy PDF term.
This leads to splitting weights that depend on the actual shape of the assumed signal flux because the sensitivity per sample can change when the flux gets harder or softer
\begin{equation}
  w_j(\vec{x}_k, \gamma_k)
  = \frac
      {
        T_j \cdot \int_{E_\nu} A_{j,\text{eff}}(E_\nu,\Omega_k)\cdot
        \phi(E_\nu, \Omega_k | \gamma_k)\,\d{E_\nu}
      }
      {
        \sum_{m=1}^{N_\text{srcs}}
        T_m \cdot \int_{E_\nu} A_{m,\text{eff}}(E_\nu,\Omega_k)\cdot
        \phi(E_\nu, \Omega_k | \gamma_k)\,\d{E_\nu}
      }
  \mperiod
\end{equation}
More details on how these weights are constructed follow in the next section.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %% General method to get a priori weights from effective areas
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A priori weight selection}
The a priori weights for the expected signal of a source in a specific sample should match the true emission model as closely as possible to gain optimal analysis performance.
Note that the weights resemble the expected relative flux at the detector.
For example, for an IceCube-like detector that only measures the northern sky, all weights for sources on the southern sky would be zero, regardless of the true intrinsic emission strength, because no signal is ever detected.

To construct the weights that actually distribute the total expected amount of signal to the expected contribution from each source, signal simulation can be used to estimate the signal detection efficiencies.
This is done by calculating the total number of expected events from each source from a given emission scenario at detector level.
Depending on the assumed emission scenario, the weights might depend on any source parameter, usually the source position, the time and the spectral emission behaviour of the sources.
For sources that are not point-like in any parameter but show a functional dependency, the detector response must be folded with the source parameter distribution to obtain the effective weight.

For the spatial weight contribution, the weights are dependent only on the source positions and the signal response of the detector at their positions.
The assumption that IceCube is right-ascension symmetric yields stacking weights that only depend on the source declinations.
For extended sources the detector response needs to be folded with the source extension density profile incorporating efficiencies from multiple detector regions.

When using a time-dependent emission scenario the number of expected events varies with the emission type.
Because the detector response to signal usually does not vary with time, only the relative differences in the time window functions matter for the stacking weights.
For example in a per-burst scenario, the amount of signal events is independent of the considered emission time scale and a time-integrated fluence is assumed rather than a time-dependent flux.
The cases regarded in this thesis both lead to a vanishing temporal dependence.
In the time-dependent analysis, a fluence scenario is considered and all sources have the same time window length, making the temporal emission equal by design.
Even with time-windows that differ in length between each other, the expected signal is equal from all bursts by definition.
In the time-independent case, all source fluxes have the same linear temporal dependence
\begin{equation}
  \int_T \phi \,\d{t} = T\cdot \phi
\end{equation}
and are expected to steadily emit for the whole sample livetime, making all integrals over the temporal part equal.
Only when considering fluxes differential in time with different emission profiles, the integral contributions differ and the stacking weights become truly time-dependent.
It is also possible to split time-dependent sources across multiple samples with this formalism.
If they are assumed to depend on a flux, then the portion of the flux PDF in each sample can be computed.
If using a per-burst scenario the time evolution information is lost.
Therefore, it is not explicitly possible to split a burst time window across multiple samples.
A fair and general approach might be to assume a uniform temporal dependency and again see how much of the integral lies in each sample.
Repeating sources in per-burst scenarios can be treated by assuming that each burst is a separate source and making sure, that time windows do not overlap.
The latter is needed because a source is not expected to emit into two bursts at the same time.
The third common scenario is that sources also vary in their energy dependence for their intrinsic flux strengths.
Usually, this happens in the form of altering the spectral index of the assumed power-law flux.
This means the detector response is additionally depending on the spectral index as the expected number of events per declination changes depending on the spectrum's steepness.

To summarize, note that the source weights should represent the estimated splitting of the single signal strength fit parameter $n_S$, reflecting the relative, expected number of events for the whole tested parameter space.
Therefore, they do not vary per event, but only per source.
It is useful to imagine what happens if the proposed source class emits signal events into the detector and what would be recorded after the detection process.

As an example, a dependency on the source positions $\vec{x}_k$ and spectral indices $\gamma_k$ of the $n_S$ splitting weight per source $k$ is assumed.
For actually calculating the signal efficiency and thus the expected number of events per source $N_k$ at detector level and for the final sample event selection from an intrinsic, linearly time-dependent neutrino flux $\phi$, the expression
\begin{equation}
  \label{equ:eff_area_analytic}
  N_k(\vec{x}_k, \gamma_k)
  = T \cdot \int_{E_\nu} \int_{\Omega_\nu} f(\vec{x}_k - \vec{x})
      A_\text{eff}(E_\nu,\Omega_\nu) \cdot
      \phi(E_\nu,\Omega_\nu | \gamma_k) \,\d{E_\nu}\d{\Omega_\nu}
\end{equation}
is used.
Note the usage of the true neutrino energies and directions, as the interest lies in the mapping of intrinsic flux strength to expected events, not in the performance of spatial or energy reconstruction algorithms.
Here, the value of $\gamma_k$ is taken to be exact and for a point-source hypothesis, the function $f(\vec{x})$ can be replaced with a $\delta$ distribution for picking only the integral contribution from the single source position.
$A_\text{eff}$ is the hypothetical detector surface area, which would yield the same number of observed events if it would detect $\SI{100}{\percent}$ of the incoming flux compared to the real detector and the applied event selection and maps an intrinsic flux to event counts detected at detector level.
The integrated $A_\text{eff}$ is typically in units $\si{\cm\squared}$, the flux $\phi$ in \si{\per\GeV\per\cm\squared\per\second\per\steradian} and the detector livetime $T$ in $\si{\second}$.
When using a time-integrated fluence $\Phi$ model, the livetime is obsolete and the number of events read
\begin{equation}
  N_k \rightarrow \frac{N_k}{T}
  \mperiod
\end{equation}
instead.
The normalized weights per source in a single sample can be constructed by
\begin{equation}
  \label{equ:general_satcking_weights}
  w_k = \frac{N_k}{\sum_{m=1}^{N_\text{srcs}} N_m}
  \mperiod
\end{equation}
This effective area formulation holds for calculating the multi sample splitting weights as well as for the expected $n_S$ splitting within a single sample, because it is the most general form of calculating the detector efficiency and the expected number of events for a given flux or fluence hypothesis.

In practice, to obtain the desired number of events for a given signal flux or fluence, simulation data is used to approximate the integral~(\ref{equ:eff_area_analytic}) using Monte Carlo integration by estimating the unknown distribution via sampling methods.
A simulation quantity called OneWeight is used to properly weight neutrino simulation to the number of expected events, being able to compare expectation values obtained from simulation directly to measured data.
The OneWeight per event $i$ is defined as \cite{Gazizov:2004va}
\begin{align}
  \label{equ:oneweight_definition}
  \text{OneWeight}_i
  &= p_\text{int}\frac{\int_\Omega\int_{E_0}^{E_1}
                       \Phi_\text{gen}(E_i)\d{E}\d{\Omega}
                       }{\Phi_\text{gen}(E_i)}A_\text{gen} \\
  &= p_\text{int}\frac{\int_{E_0}^{E_1}\Phi_\text{gen}(E_i)\d{E}}
                      {\Phi_\text{gen}(E_i)}
    A_\text{gen}\Omega_\text{gen}
  \mcomma
\end{align}
where $p_\text{int}$ is the interaction probability for neutrinos forced to interact close to the detection volume \footnote{Otherwise the simulation efficiency would be very low due to the small interaction probabilities of neutrinos.}, $\Phi_\text{gen}$ is the energy fluence used to generate the initial neutrino distribution, usually a power law and $A_\text{gen}$, $\Omega_\text{gen}$ are the constant surface and the solid angle over which the initial neutrinos are injected.
In the last step, the integration over $\Omega$ is carried out because usually, the injection fluence is only energy dependent.
By this definition, OneWeight is the inverse of the generating fluence in $\si{\GeV\steradian\cm\squared}$ combined with the correction factor for the forced neutrino interaction close to the detection volume.
To obtain the number of equivalent data events for a given simulation set, the per event weights
\begin{equation}
  N = \sum_i w_i = \sum_i \frac{\Phi_i}{\Phi_{i,\text{gen}}}
\end{equation}
need to be summed up.
When using OneWeight to express the generating flux, the number of events can be obtained via
\begin{align}
  N &= T \cdot \sum_i \frac{\text{OneWeight}_i \cdot \phi_i}{N_\text{gen}} \\
  \intertext{or for fluences without the livetime $T$}
  N &= \sum_i \frac{\text{OneWeight}_i \cdot \Phi_i}{N_\text{gen}}
  \mcomma
\end{align}
where $N_\text{gen}$ is the generated total number of events summed over both particle and anti-particle types.
This scaling makes sure, that the physical expectation value stays the same, regardless of the number of simulated events.
The sum can also be taken across subsamples of a whole data set to obtain differential event counts, e.g. per energy or solid angle to obtain approximations for the analytic integration in formula~(\ref{equ:eff_area_analytic}).

\subsection{Intrinsic source weights}
Additional intrinsic source weights can be introduced to capture differences in the expected, relative fluxes or fluences from the sources themselves.
This is decoupled from the actual detection mechanism and the detector efficiency weights expressed by formula~(\ref{equ:general_satcking_weights}).
The weight selection strongly depends on the used catalogue or source category and can, for example, be a gamma-ray flux or distance weighting \cite{Huber:2017wxt,Aartsen:2016lir}.
These intrinsic source weights $w_k^\text{src}$ are independently multiplied with the corresponding detector weights $w_k^\text{det}$ to form the total source weights used in the stacking
\begin{equation}
  w_k \coloneqq w_k^\text{det} \cdot w_k^\text{src}
  \mperiod
\end{equation}
If the intrinsic source properties are not known or not reliably available, the intrinsic source weights are usually assumed to be equal to trade a potentially worsened performance for avoiding to much model specific bias.

\subsection{Connecting weighting formulas}
The connection to the effective area formula~(\ref{equ:eff_area_analytic}) with the more practical one using simulation weights is shortly depicted below.
From the definition of the effective area
\begin{equation}
  A_\text{eff}(\Delta E, \Delta \Omega) =
    A_\text{gen} \frac{\hat{N}(\Delta E, \Delta \Omega)}
                      {\hat{N}_\text{gen}(\Delta E, \Delta \Omega)}
  \mcomma
\end{equation}
where $\Delta E$ and $\Delta \Omega$ are arbitrary chosen integration intervals in energy and solid angle, because the effective area is only properly defined under the integral, the number of generated events per interval can be obtained from the flux or fluence assumption used to sample the primary simulation particles
\begin{equation}
  \phi_\text{gen} = \phi_0 \cdot f(E, \Omega)
  \mperiod
\end{equation}
The unit-less functional flux dependency is described in the usually energy and solid angle dependent function $f(E, \Omega)$.
Because the generating function is usually uniform and independent in solid angle it follows that $f(E, \Omega) \rightarrow f(E)\cdot \sfrac{1}{\Omega_\text{gen}}$.
The expected number of simulated primaries in the interval is then given by
\begin{equation}
  \hat{N}_\text{gen}(\Delta E, \Delta\Omega) =
    N_\text{gen} \cdot \frac{\Delta\Omega}{\Omega_\text{gen}}
    \frac{\int_{E'}^{E'+\Delta E} f(E) \,\d{E}}
         {\int_{E_0}^{E_1} f(E) \,\d{E}}
  \mperiod
\end{equation}
For neutrino simulation, where primaries are forced to interact close to the detection volume, the number of events is given by
\begin{equation}
  \hat{N}(\Delta E, \Delta \Omega)
  = \sum_{i=1}^{N_\text{gen}} w_{i,\text{sel}}\cdot p_{i,\text{int}}
  = \sum_{(i|E_i\in\Delta E, \Omega_i\in\Delta\Omega)} p_{i,\text{int}}
  \mcomma
\end{equation}
where $w_{i,\text{sel}}$ is either $0$, if the event is not present or $1$ if it is present in the final selection in bin $\Delta E$, $\Delta \Omega$.
Now replacing $\hat{N}_\text{gen}$ and $\hat{N}$ in the effective area definition yields
\begin{equation}
  A_\text{eff}(\Delta E, \Delta \Omega) =
    A_\text{gen} \cdot
    \left(\sum_{(i|E_i\in\Delta E, \Omega_i\in\Delta\Omega)}
          p_{i,\text{int}}\right) \cdot
    \frac{\Omega_\text{gen}}{\Delta\Omega} \cdot \frac{1}{N_\text{gen}}
    \frac{\int_{E_0}^{E_1} f(E) \d{E}}
         {\int_{E'}^{E'+\Delta E} f(E) \d{E}}
  \mperiod
\end{equation}
Identifying and replacing the OneWeight definition from~(\ref{equ:oneweight_definition}), the formula to obtain the effective area estimate from a simulation data set becomes
\begin{equation}
  A_\text{eff}(\Delta E, \Delta \Omega) =
    \frac{1}{N_\text{gen}\Delta\Omega}\cdot
    \frac{\left(
            \sum_{(i|E_i\in\Delta E, \Omega_i\in\Delta\Omega)}
            \text{Oneweight}_i \cdot f(E_i)
          \right)}
          {\int_{E'}^{E'+\Delta E}f(E)\d{E}}
  \mperiod
\end{equation}
Sometimes a slightly modified version
\begin{equation}
  A_\text{eff}(\Delta E, \Delta \Omega) =
    \frac{1}{N_\text{gen}\Delta\Omega}\cdot
    \frac{\sum_{(i|E_i\in\Delta E, \Omega_i\in\Delta\Omega)}
          \text{Oneweight}_i}
         {\Delta E}
\end{equation}
is used, which is only valid for small integration intervals, because it assumes an approximately constant generating function $f$ within an interval by simplifying
\begin{equation}
  \frac{f(E_i)}{\int_{E'}^{E'+\Delta E}f(E)\d{E}}
  \approx \frac{f(E_i)}{f(E')\cdot\Delta E}
  \approx \frac{f(E_i)}{f(E_i)\cdot\Delta E}
  = \frac{1}{\Delta E}
  \mperiod
\end{equation}


\section{Frequentist methods}
  \label{chp:pointsource_frequentist}
In this thesis, significances for hypothesis tests are obtained using frequentist methods.
This section shortly introduces the relevant terminology and is a summary of the extensive treatments in \cite{casella2002statistical,blobel2013statistische,barlow1989statistics}.
The methods used are \emph{point estimation} and \emph{interval estimation}.
Usually, the result of a model-dependent point source search, as done here, is a single best fit result.
This best fit result is the point in the considered space of free parameters that gives the most likely model to describe the measured data.
This is called a point estimate.
To also obtain an estimate of the region of the true underlying physics parameters, an interval estimation needs to be done.

Point estimation is directly connected to \emph{hypothesis testing} because for a given best-fit result, the significance of being compatible with the null hypothesis is wanted, to see if the alternative model is favoured.
Therefore, the general procedure is to set the null hypothesis to a background-only assumption or the current understanding of the underlying physics.
For a given point estimate, an incompatible result can give reason to reject the null hypothesis and thus open the possibility to claim new discoveries.
A p-value $p$ gives the probability to obtain the given or a larger test statistic value under the assumption that the null hypothesis holds
\begin{equation}
  p \coloneqq \int_x^\infty f_{H_0}(-2\ln\lambda) \d(x)
  \mcomma
\end{equation}
where $f_{H_0}(-2\ln\lambda)$ is the test statistic distribution under the null hypothesis $H_0$.
The test statistic is generated by doing pseudo experiments by drawing samples from a distribution that well resembles the parameter space of the null hypothesis.
A small p-value indicates a strong deviation from the test result from the expected behaviour of the null hypothesis.
It does not imply the truth of the alternative hypothesis though, but only that it is unlikely for the data to originate from the null hypothesis distribution.

To obtain further information on the possible value of the true parameter, an interval estimation needs to be done.
In general, a frequentist confidence region $[\theta_0, \theta_1]$ for a parameter $\theta$ at a confidence level $\alpha$ is defined to have the property of \enquote{covering the true parameter $\hat{\theta}$ in a fraction $\alpha$ of random measurements} \cite{casella2002statistical}.
Note, that the interval itself is a random quantity and may or may not contain the true parameter.
Only a statement about ensembles of measurements can be made.
By following the definition for a consistent construction of the intervals, it is ensured the desired coverage probability is reached.

A general method to construct confidence intervals is the \emph{Neyman construction} \cite{Neyman:1937cls}.
In a modern language, the method relies on the sampling of the probability space of the defined parameter space, creating sets of samples for which the true parameters are known.
For example, a single parameter $n_S$ is fitted in a point source Likelihood, which is an estimator for the true, but unknown signal strength $\mu$.
Using a set of experimental data $X=(x_1, \dots, x_N)$, a best-fit point estimator can be obtained by finding the parameter $\hat{n}_S$, that maximises the Likelihood
\begin{equation}
  \mathcal{L}(n_S) = \prod_{i=1}^N P(n_s|x_i)
  \mperiod
\end{equation}
By performing pseudo experiments and creating samples for which the true parameter is scanned in a region of interest, a two-dimensional plane of the true and estimated parameter can be created for the example case here.
To construct the confidence interval for $\mu$, first, an interval containing the probability $\alpha$ is created on the sampled statistic for each parameter $\mu_i$.
This ensures a consistent treatment of the enclosed probabilities according to the definition.
Then the confidence interval for $\mu$ from the single point estimate $\hat{n}_S$ at a confidence level $\alpha$ is the set of all $\mu_i$, which have the measured value $\hat{n}_S$ in the previously built interval.
Using the histogram analogy, with $n_S$ on the x-axis and $\mu$ on the y-axis, the interval $[\mu_0, \mu_1]$ is the vertical line that cuts through the band created by marking the horizontal interval start and end points for each value $\mu_i$.

Interval estimation is closely related to hypothesis testing.
Hypothesis testing is used to find the acceptance region for a fixed set of model parameters by probing all possible sample values.
In interval estimation, the sample is fixed and the question is, which model parameters plausibly explain the data in the whole parameter space.
This can be expressed schematically with
\begin{equation}
  (x_1, \dots, x_N) \in A(\theta_0)
  \Leftrightarrow
  \theta_0 \in C(x_1, \dots, x_N)
  \mcomma
\end{equation}
with a function $A$ describing the acceptance region for the sample $X=(x_1, \dots, x_N)$ and a function $C$ describing the confidence region for the true parameter $\theta_0$.
