\chapter{Point source searches with IceCube neutrinos}
In this chapter, the unbinned Likelihood methods used for multiple IceCube point source searches are derived.
To identify an astrophysical neutrino signal from a specific source location on the sky an excess of events from that direction needs to be identified.
In general, it cannot be distinguished between atmospheric and the sought after extraterrestrial neutrinos on a per event basis.
Instead, it is possible to search from a sample based point-of-view by measuring deviations for an ensemble of events with respect to a known background expectation.
A simple method could be to define a fixed search region around a direction from which signal is expected, count the number of measured events and compare them to the number of events from a background expectation in the region.
\CITE{Li Ma Paper.}
If the measured number of events is significantly higher than the expected number of background events that may be a hint for a signal from that direction.

Here the followed approach is similar but used in a more advanced form, incorporating multiple pieces of event information to increase the detection sensitivity.
Also the usage of pre-defined search regions, often also called a binned approach, is replaced with an unbinned version on a per event basis.
This has the advantage of avoiding hard search region boundaries which can drop the sensitivity of the approach if e.g. an unknown source lies directly at the border of such a region.
Starting from a general unbinned, extended Likelihood approach, the special cases used in this analysis to carry out a time dependent stacking search for point-like sources are derived.
Particular cases handling multiple years of data from different detector configurations and multiple sources for the so called stacking case are derived from the basic form.
In the following a single event is noted with index $i$, a source with index $k$ and a data sample with index $j$.

Significances are obtained using frequentist methods in this thesis \CITE{Some stats book, Fischer?}.
A p-value $p$ gives the probability to obtain the given or a larger test statistic value under the assumption that the null hypothesis holds
\begin{equation}
  p \coloneqq \int_x^\infty f_{H_0}(-2\ln\lambda) \d(x)
\end{equation}
where $f_{H_0}(-2\ln\lambda)$ is the test statistic distribution under the null hypothesis $H_0$.
A small p-value thus indicates a strong deviation from the test result from the expected behaviour of the null hypothesis.
It does not imply the truth of the alternative hypothesis though, but only that it is so unlikely for the data to originate from the null hypothesis distribution \CITE{Casella Berger}.

\section{Extended unbinned Likelihood}
The extended Likelihood \CITE{Barlow book} and the corresponding logarithmic extended Likelihood function is defined as
\begin{equation}
  \mathcal{L}(\lambda) = \frac{\lambda^N e^{-\lambda}}{N!}\prod_{i=1}^N P_i
  \quad\Rightarrow\quad
    \ln\mathcal{L}(\lambda) = -\lambda+\sum_{i=1}^N \ln(\lambda P_i)
  \mcomma
\end{equation}
where the constant term $\ln(N!)$ is dropped in the logarithmic version.
Here $\lambda$ is the expected number of events and $N$ the number of measured events following a Poisson distribution.
The per event model distribution $P_i$, normalized to integral $1$ over the defined parameter space, describes the Likelihood of each event under the assumed model and how likely it contributes to the expectation.
The use of the Poisson term is justified by a re-normalization of the per event distributions to include the total number of measured events which is not fixed for multiple experiments of the same kind but may fluctuate around an unknown expectation value.

The tested hypotheses are encoded in the description of the models $P$.
To obtain a fairly general expression to derive the point source special cases from, the expectation model can be split into multiple classes by splitting the expectation and the models accordingly
\begin{equation}
  \lambda = \sum_{k=1}^{N_\text{classes}} \lambda_k \geq 0
  \mintertext{and}
  P_i = \frac{1}{\sum_{k=1}^{N_\text{classes}} \lambda_k}\cdot
         \sum_{k=1}^{N_\text{classes}} \lambda_k P_{i,k}
  \mperiod
\end{equation}
The single $\lambda_k$ can be negative but their sum must not, because it is still a Poisson expectation parameter.
Additionally, the new split model is normalized over all classes to arrive at the form
\begin{equation}
  \ln\mathcal{L}(\{\lambda_k\})
  = -\sum_{k=1}^{N_\text{classes}} \lambda_k +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{classes}}
      \lambda_k P_{i,k} \right)
  \mperiod
\end{equation}

To specialize more, it is usually desired to test a signal hypothesis against a background one, for $N_\text{srcs}$ sources in general, for each event $i$.
The above expression is thus expanded to include $N_\text{srcs}$ signal and $N_\text{srcs}$ background parameters and the corresponding distributions $S_{i,k}$ and $B_{i,k}$:
\begin{equation}
  \ln\mathcal{L}(\{\lambda_{k,S}\}, \{\lambda_{k,B}\})
  = -\sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,S}+\lambda_{k,B}\right) +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{srcs}}\left(
      \lambda_{k,S} S_{i,k}+\lambda_{k,B} B_{i,k}\right)\right)
  \mcomma
\end{equation}
and from the Poisson condition there is still the constrain
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,S}+\lambda_{k,B}\right) \geq 0
  \mperiod
\end{equation}

For testing the significance of a potential signal contribution in the measured data, a Likelihood ratio test is used.
The null hypotheses $H_0$, which is that only background is expected to be measured, is constructed by using only a portion $\Theta_0$ of the allowed parameter space, here by setting all signal expectations to zero
\begin{equation}
  \ln\mathcal{L}_0(\{\lambda_{k,S}=0\}, \{\lambda_{k,B}\})
  = -\sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,B}\right) +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{srcs}}\left(
      \lambda_{k,B} B_{i,k}\right)\right)
  \mperiod
\end{equation}
The alternative hypothesis $H_1$ is constructed by using the full Likelihood parameter space $\Theta$:
\begin{equation}
  \ln\mathcal{L}_1(\{\lambda_{k,S}\}, \{\lambda_{k,B}\})
  = -\sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,S}+
                                     \lambda_{k,B}\right) +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{srcs}}\left(
      \lambda_{k,S} S_{i,k}+\lambda_{k,B} B_{i,k}\right)\right)
  \mperiod
\end{equation}

The Likelihood ratio test statistic $\Lambda$ for testing the null hypothesis $H_0$ against the alternative $H_1$ is defined as \CITE{Casella Berger Book}
\begin{equation}
  \ln\hat{\Lambda}
  = \ln\left(\frac{\sup_{\theta \in \Theta_0} \mathcal{L}(\theta)}
                  {\sup_{\theta \in \Theta} \mathcal{L}(\theta)}\right)
  = \ln\left(\sup_{\theta \in \Theta_0} \mathcal{L}(\theta)\right) -
    \ln\left(\sup_{\theta \in \Theta} \mathcal{L}(\theta)\right)
  \mperiod
\end{equation}
where $\hat{\Lambda}$ means the single test statistic value after finding the supremum of both nominator and denominator.
This leads to the test statistic
\begin{equation}
  \begin{aligned}
    -2\ln\hat{\Lambda}
    &= 2\ln(\mathcal{L}_1(\{\hat{\lambda}_{k,S/B}\})) -
       2\ln(\mathcal{L}_0(\{\hat{\lambda}^{(0)}_{k,B}\})) \\
    &= -2\left(\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} +
                                         \hat{\lambda}_{k,B} -
                                         \hat{\lambda}_{k,B}^{(0)}\right) +
      2\sum_{i=1}^N \ln\left(
        \frac{\sum_{k=1}^{N_\text{srcs}}\left(
            \hat{\lambda}_{k,S} S_{i,k}+\hat{\lambda}_{k,B} B_{i,k}\right)}
            {\sum_{k=1}^{N_\text{srcs}}\left(
              \hat{\lambda}_{k,B}^{(0)} B_{i,k}
            \right)}
          \right)
    \mcomma
  \end{aligned}
\end{equation}
which has been decorated by the factor $-2$ to be compatible to Wilks' theorem \CITE{Wilk paper and/or statistic book}.
Here the parameters $\hat{\lambda}_{k,S/B}$ were introduced, which mean the parameters $\lambda_{k,S/B}$ that maximize the Likelihood $\mathcal{L}_1$ under the complete parameter space and $\hat{\lambda}_{k,B}^{(0)}$ maximizing $\mathcal{L}_0$.
As seen above, all best-fit parameters from both hypotheses have to be distinguished in general, differentiating $\hat{\lambda}_{k,B}^{(0)}$ from the null hypothesis which is not the same as $\hat{\lambda}_{k,B}$ from the composite hypothesis.

In the following sections, specific model choices for the signal and background distributions and approximating assumptions are shown, to transform the above expression to commonly known forms also used for the time-dependent search in this thesis.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %% General method to get the PDFs, not the used implementation
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Per event distributions}
The introduced per event distributions $S_i, B_i$ are the functions that actually define the tested hypothesis of the analysis.
Depending on their structure they can describe a single point source search, a stacking search for searching at various source positions at once or a template search, where a whole spatial region is tested for neutrino emission over the expectation.
The used PDFs are usually similar for all analysis types and the conventions applied for most point source searches in IceCube are followed here \CITE{Braun paper. Maybe both for time dep. / indep. searches.}.

The per-event distributions describe the main separation power between signal and background hypotheses in combination with the mixing portions $\lambda_{i,S/B}$ by introducing a-priori knowledge in defining signal- and background-like regions in the tested parameter space.
The better these distributions are able to separate signal and background regions the more sensitive the analysis becomes.
A common approach with known good separation power is to combine contributions from spatial clustering and energy information, where the first one is necessary for the tested hypotheses and the latter providing additional information under certain assumptions of signal flux shapes.
For time dependent analyses an extra time dependent part is introduced.

For most IceCube point source searches, the signal and background contributions can be written as independent products of a spatial, an energy and a time dependent part as
\begin{align}
  S_{i,k}
    &= S(\vec{x}_i, \vec{x}_{\mathrm{src},k}, E_i | \gamma)
     = S^S(\vec{x}_i, \vec{x}_{\mathrm{src},k}) \cdot
       S^E(E_i, \delta_i | \gamma) \cdot
       S^T(t_i, t_k) \\
  \intertext{and}
  B_{i,k}
    &= B(\delta_i, E_i | \phi_\mathrm{BG})
     = B^S(\delta_i) \cdot
       B^E(E_i, \delta_i | \phi_\mathrm{BG}) \cdot
       B^T(t_i, t_k)
\end{align}
where $\gamma$ is the shape parameter of a signal flux usually assumed to be a power law $\propto E^{-\gamma}$ and $\phi_\mathrm{BG}$ stands for a flux model of the atmospheric neutrino flux describing the background flux dependency.
The event times $t_i$ and source times $t_k$ define the time dependent emission-model.
Note that the PDFs defined above are used in various analysis but neglect some correlations, which will be explained in more detail in this chapter.
Also note that only variables known on experimental data can be used and no simulation ground truth is known on actual data.

\subsection{Spatial distribution}
The most important part in the search for neutrino point sources is the spatial clustering of events around a point in the sky.
Without the spatial term, only a insensitive diffuse analysis would be performed.
For data samples used in point source searches, there is a reconstructed estimate of the per event uncertainty available.
This estimator is built from the positional reconstruction Likelihood fit and is constructed assuming a symmetric, two-dimensional Gaussian distribution describing the reconstruction uncertainty.
Alternatively, the same value can be transformed and used for the symmetric Kent distribution \CITE{Kent paper} for the per event spatial distribution instead.
The van-Mises distribution is the pendant of a symmetric two dimensional Gaussian but correctly normalized on the sphere and is useful for larger uncertainties for example when investigating cascade-like events.
For tracks, the good angular resolution of about \SI{1}{\degree} justifies the use of the simpler and more familiar Gaussian distribution as both distributions become virtually indistinguishable for small uncertainties.
\TODO{Appendix: Show or ref to plot of sigma vs kappa}

In general, the joint probability for an event $i$ to spatially originate from a source $k$ can be obtained by a convolution of the two separate spatial PDFs.
One part describes the intrinsic distribution for the source and the other part the intrinsic distribution of the event itself.
In a search for point sources at known positions, the distribution of a source position is represented by a delta distribution, in searches for spatially extended sources usually a Gaussian profile is assumed.
The per-event probability distribution is assumed to be a Gaussian PDF, which is connected to the construction of the per-event uncertainty from a parabolic Likelihood space or a bootstrap algorithm.
The resulting spatial signal distribution describing the probability of an event being spatially correlated to a given source with density profile $f(\vec{x}_k)$ can thus be expressed as a convolution of the Gaussian per event part and source distribution for the fixed source position
\begin{equation}
  S^S(\vec{x}_i, \vec{x}_k)
  = \int_\Omega \frac{1}{2\pi \sigma_i^2}
    \exp\left(-\frac{|\vec{x}-\vec{x}_i|^2}{2\sigma_i^2}\right) \cdot
    f(\vec{x}_k - \vec{x}) \d{\vec{x}} \\
    \mperiod
\end{equation}
For the point-source hypothesis, $f(\vec{x}_k)$ can be replaced with a delta distribution $\delta(\vec{x}_k)$ yielding the often used expression
\begin{equation}
  S^S(\vec{x}_i, \vec{x}_k)
  = \frac{1}{2\pi \sigma_i^2}
    \exp\left(-\frac{|\vec{x}_k-\vec{x}_i|^2}
                    {2\sigma_i^2}\right)
\end{equation}
for the spatial signal PDF.
Assuming extended sources with a Gaussian density profile would follow the same scheme and an analytic solution for the convolution of two Gaussian distributions exists, resulting in a substitution of $\sigma_i \rightarrow \sqrt{\sigma_i^2 + \sigma_k^2}$ \CITE{Gaussian convolution}.

The spatial background distribution is constructed similarly to the signal case above.
Because IceCube is a nearly right-ascension symmetric detector at the south pole, the distribution is assumed to be declination dependent only.
This may break down for time scales so small, that the earth's rotation doesn't smear out the slightly asymmetric distribution in azimuth angle, which can then be used as a drop-in replacement, but holds better the larger the regarded time windows get.
This dependency is written in a general form here and can, for example, be constructed by computing histograms of experimental data, which is described later in more detail.
The spatial background distribution can then be written as
\begin{equation}
  B^S(\delta_i) = \frac{1}{2\pi} \cdot P(\sin(\delta_i))
  \mcomma
\end{equation}
where the first factor is the uniform distribution in right-ascension and the latter indicating, the often alternatively used, dependence on the sine of declination $\delta$.
Note that the background distribution is only depending on the event position because background should have no correlation with any source by definition.
This description neglects the per-event uncertainty though.
In principle the same method can be applied for the background as was used to construct the convoluted signal PDF.
With
\begin{equation}
  B^S(\vec{x}_i) =
    \int_\Omega \hat{B}^S(\vec{x}) \cdot
      \exp\left(-\frac{|\vec{x}-\vec{x}_i|^2}{2\sigma_i^2}\right)
      \,\d{\vec{x}}
\end{equation}
the per-event distribution would be described in full detail.
However, as the intrinsic background PDF $\hat{B}^S$ does not change much with declination and a closed form analytic description as in the Gaussian case above is not easily possible with $\hat{B}^S$ derived from histograms, the per-event uncertainty is neglected.


\subsection{Energy distribution}
In addition to the spatial clustering, the event's energy can also provide a powerful separation argument and lead to a large improvement in sensitivity \CITE{Braun Paper}.
As the energy density of atmospheric neutrinos can approximately be described by a power law $\phi_\mathrm{BG}(E) \propto E^{-3.7}$ and astrophysical signal by a harder spectrum around $\phi(E)_S \propto E^{-2}$, higher energy events are more likely to originate from an extraterrestrial source rather than having been created in the atmosphere.
In the PDFs, the energy dependence can only be taken into account with energy estimators of the true neutrino energy $E_\nu$ because the information must also be available on measured data, not knowing the true neutrino energy.
Formally, that mapping can be written as an integration using the law of total probabilities of the distribution of the energy proxy of the event with the probability of obtaining a true neutrino energy under the current flux hypothesis
\begin{equation}
  \hat{S}^E(E, \vec{x}|\gamma) =
    \int_{E_\nu}\int_{\Omega_\nu}
    P(E, \vec{x}|E_\nu, \vec{x}_\nu) \cdot P(\vec{x}_\nu, E_\nu|\gamma)
    \,\d{E_\nu}d{\vec{x}_\nu}
    \mcomma
\end{equation}
where $\gamma$ is the shape parameter of an assumed signal power law flux.

For background, the same reasoning applies and the flux model is substituted for one describing the atmospheric neutrino background instead
\begin{equation}
  \hat{B}^E(E, \vec{x}|\phi_\mathrm{BG}) =
    \int_{E_\nu}\int_{\Omega_\nu}
    P(E, \vec{x}|E_\nu, \vec{x}_\nu) \cdot P(\vec{x}_\nu, E_\nu|\phi_\mathrm{BG})
    \,\d{E_\nu}d{\vec{x}_\nu}
\end{equation}
to obtain the intrinsic distribution.

In practice, the integrals may be obtained using histograms in reconstructed declination and an energy estimator from simulations for signal and from either simulations or measured data for the background distributions.

To obtain the convolved PDFs that also take into account the per event uncertainty for each event in energy and position, the same ansatz as above can be used
\begin{align}
  S^E(E_i, \delta_i|\gamma) &=
    \int_E\int_\Omega f(\vec{x}_i, E_i)\hat{S}^E\,\d{E}\d{\vec{x}} \\
  B^E(E_i, \delta_i|\phi_\mathrm{BG}) &=
    \int_E\int_\Omega P(E_i,\delta_i|E_\nu)\cdot P(E_\nu|\phi_\mathrm{BG})
      \d{E_\nu}
    \mperiod
\end{align}
In the majority of analysis, a $\delta$ distribution for the per-event functions is used in the last step, for the same reasons quoted for the spatial background PDF.
Also the distribution is normalized independently of the spatial PDFs, although the declination occurs in both of them.
This may be done to simplify the computation and housekeeping in analysis code, because it avoids higher dimensional PDF constructions.

\subsection{Time dependency}
When testing for time dependent emission models, time dependent PDFs $S_{i,k}^T(t_i, t_k)$ and $B_{i,k}^T(t_i, t_k)$ need to be incorporated, which depend on each events time and their occurrences relative to the sources' temporal evolution.
The assumption is then, that each source only emits neutrinos as given by $S_{i,k}^T(t_i, t_k)$.
The background can in general also be time dependent to account e.g. for seasonal variations in the detector rate.
Because of the precise timing information, there is no relevant time uncertainty that needs to be convolved into the intrinsic PDFs.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %% Special case GRB LLH
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Time dependent Likelihood}
To test for time dependent emission in this analysis, the general extended Likelihood is altered to a form similar to what is usually called \emph{Gamma Ray Burst Likelihood}\footnote{Named after the original purpose of searching for emission from a Gamma Ray Burst catalogue. The concept can be applied to other sources testing a similar hypothesis of emission within a defined time window.} \CITE{Mikes thesis or GRB paper or FRB paper}.
As the name may indicate, the source emission is assumed to be per-burst with a fluence $\Phi$ rather than a time-dependent flux.

This includes explicit assumptions about the temporal source emission PDFs and simplifications of the general Likelihood formula introduced before.
The simplifications need to be applied mainly because when choosing small time windows the analysis deals with a very low amount of leftover events.
Therefore the number of free parameters needs to be reduced as much as possible without introducing too many a priori assumptions to stick to a rather general search method because the source types themselves are unknown.

Here, using the burst model, a rather general assumption about the source emission is made.
The time PDFs are therefore constructed using rectangle functions
\begin{equation}
  \rect(t)
  = \Theta\left(t+\frac{1}{2}\right) \cdot \Theta\left(t-\frac{1}{2}\right)
  = \begin{cases}
      1 &\text{ , if}\quad |t| \leq \frac{1}{2} \\
      0 &\text{ , if}\quad  t    >  \frac{1}{2} \\
    \end{cases}
  \mcomma
\end{equation}
where $\Theta$ is the Heaviside step function \CITE{AbramovitzStegun}.
This way, each source is having only a non-zero emission contribution within its pre-defined source time window
\begin{equation}
  S_{i,k}^T = B_{i,k}^T = T_k(t_i) \coloneqq
    \rect\left(\frac{t_i - \frac{t_k^1-t_k^0}{2}}
                              {t_k^1-t_k^0}\right)
  \mperiod
\end{equation}
This effectively cuts out a subset of events around the sources time stamps $t_k$ and the corresponding time interval around each source $[t_k^0, t_k^1]$ and is a background rejection technique only, because any underlying temporal dependence on the source emission flux is discarded.
The most simple case is then to have each source in its own, non-overlapping time window so that each source has a unique set of events belonging to it, which is the case in this analysis.
Note again that no separation-power stems from these time PDFs, they are merely used to reduce the background rate under the assumption of a temporally concentrated emission.

One important simplification of the general Likelihood that is applied is that the background expectations are not fitted, but rather fixed from the integrated off-time data rate over the range of the background time PDFs.
This decreases the number of parameters to fit for because it unifies and fixes the background estimators to
\begin{equation}
  \hat{\lambda}_{k,B} = \hat{\lambda}_{k,B}^{(0)} = \Braket{\lambda_{k,B}}
  \mperiod
\end{equation}
The test statistic then turns to the form
\begin{equation}
  -2\ln\hat{\Lambda}
  = -2\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} +
      2\sum_{i=1}^N \ln\left(
        \frac{\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} S_{i,k}}
             {\sum_{k=1}^{N_\text{srcs}}\Braket{\lambda_{k,B}} B_{i,k}}
        + 1
      \right)
  \mperiod
\end{equation}

The last Likelihood simplification performed, in the necessity to that a large number of free parameters cannot be fitted to very few events in a low background analysis, is to fix the relative signal expectations of each source a priori and only fit for the total expectation
\begin{equation}
  \lambda_{k,S} = n_S \cdot w_k
\end{equation}
with a free global signal strength parameter $n_S$ and weights $w_k$ normalized so that
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}} w_k = 1
  \mperiod
\end{equation}
The test statistic can then be expressed by
\begin{equation}
  -2\ln\hat{\Lambda}
  = -2\hat{n}_S +
      2\sum_{i=1}^N \ln\left(
        \frac{\hat{n}_S \sum_{k=1}^{N_\text{srcs}} w_k S_{i,k}}
             {\sum_{k=1}^{N_\text{srcs}}\Braket{\lambda_{k,B}} B_{i,k}}
        + 1
      \right)
  \mperiod
\end{equation}

The a priori fixed weights resemble the expectation from every single source at the detector.
They can, therefore, depend on the explicitly chosen source emission model and the detection efficiency depending on the source location in the detector.
Note that the a-priori chosen weights should match the true, but usually unknown, emission scenario as much as possible to obtain a good analysis sensitivity.
If the true scenario strongly differs from the assumed weights, very little can be obtained from the analysis \CITE{Rameez thesis}.


\subsection{Single sample stacking case}
Shown here is, that the assumption of independent time windows for the sources simplifies the test statistic further.
The expression can be rearranged to an explicit sum of logarithms, that better resembles the uniqueness of each source in its time window.

Starting from the test statistic from above
\begin{equation}
  -2\ln\hat{\Lambda}
  = -2\hat{n}_S +
      2\sum_{i=1}^N \ln\left(
        \frac{\hat{n}_S \sum_{k=1}^{N_\text{srcs}} w_k S_{i,k}}
             {\sum_{k=1}^{N_\text{srcs}}\Braket{\lambda_{k,B}} B_{i,k}}
        + 1
      \right)
  \mcomma
\end{equation}
and from the definition of the unique time windows, it can be seen that each event $i$ can only contribute to a single source $k$.
The event belongs to the source in which time window it falls or to no source at all, where the time windows are defined as above
\begin{equation}
  T_k(t_i) \coloneqq \rect \left(
    \frac{t_i - \frac{t_k^1-t_k^0}{2}} {t_k^1-t_k^0}
  \right)
  \mperiod
\end{equation}
The stacking sum then turns to
\begin{align}
  -2\ln\hat{\Lambda}
  &= -2\hat{n}_S +
      2\sum_{i=1}^N \ln\left(
        \frac{\hat{n}_S \sum_{k=1}^{N_\text{srcs}} w_k S_{i,k}
              \delta_{\{i,k|T_k(t_i)\neq 0\}}}
             {\sum_{k=1}^{N_\text{srcs}}\Braket{\lambda_{k,B}} B_{i,k}
              \delta_{\{i,k|T_k(t_i)\neq 0\}}}
        + 1
      \right) \\
  &= -2\hat{n}_S +
      2\sum_{i=1}^N \ln\left(
        \frac{\hat{n}_S \left[0+\dots+0+ w_{k^*} S_{i,k^*} +0+\dots+0\right]}
             {\left[
              0+\dots+0+ \Braket{\lambda_{k^*,B}} B_{i,k^*} +0+\dots+0
              \right]}
        + 1
      \right) \\
  &= -2\hat{n}_S +
      2\sum_{i=1}^N \sum_{k=1}^{N_\text{srcs}} \ln\left(
        \frac{\hat{n}_S w_k S_{i,k}}{\Braket{\lambda_{k,B}} B_{i,k}}
        + 1
      \right)
  \mcomma
\end{align}
where $k^*$ means the $k$ that fulfils the condition $T_k(t_i)\neq 0$ and in the last step it is used that
\begin{equation}
  \ln\left(
      \frac{\hat{n}_S w_{k\neq k^*} S_{i,k\neq k^*}}
           {\Braket{\lambda_{k\neq k^*,B}} B_{i,k\neq k^*}}
      + 1
    \right)
    = \ln(0 + 1) = 0
  \mperiod
\end{equation}

\subsection{Multiple samples stacking case}
To add more sources to the unique time window scenario, data at the times at which the added source events occurred needs to be taken into account.
Because after each year of taking IceCube data there may be a change in the data-taking-procedure, these changes must be included in the expectations for the samples used in the single Likelihood test.
These differences can be considered in an additional weighting scheme.
The reasoning is quite similar to the stacking case for the single sample Likelihood from before.

To add another sample, the individual Likelihoods can be summed up, because the tested datasets are independent, which yields the combined test statistic
\begin{align}
  -2\ln\hat{\Lambda}
  &= \sum_{j=1}^{N_\text{sam}} -2\ln\hat{\Lambda}_j(\hat{n}_{S,j}) \\
  &= \sum_{j=1}^{N_\text{sam}} \left[
        -2\hat{n}_{S,j} +
        2\sum_{i=1}^{N_j} \sum_{k=1}^{N_\text{srcs,j}} \ln\left(
          \frac{\hat{n}_{S,j} w_{k,j} S_{i,k,j}}
               {\Braket{\lambda_{k,j,B}} B_{i,k,j}}
          + 1
        \right)
      \right]
  \mcomma
\end{align}
where individual free $n_{S,j}$ signal parameters are introduced and the stacking weights $w_{k,j}$ per source in the sample are constructed like the usual single sample stacking weights $w_k$ and are still normalized per sample
\begin{equation}
  \sum_{k=1}^{N_{j,\text{srcs}}} w_{k,j} = 1
  \mperiod
\end{equation}
Only the number of events per sample $N_j$ and the sources falling in sample $j$ are considered for the Likelihood, weights $w_{k,j}$, PDFs $S_{i,k,j}$ and $B_{i,k,j}$ and the background expectations per source $\Braket{\lambda_{k,j,B}}$ for each sample.
Again, a-priori information about the expected number of signal events originating from each sample can be used and a global free signal strength parameter $n_S$ is used with
\begin{equation}
  n_{S,j} = w_j n_S
  \mperiod
\end{equation}
To calculate the a-priori weights $w_j$ the law of total probability is applied \CITE{Casella Berger}
\begin{equation}
  w_j = P(j) = \sum_{k=1}^{N_\text{srcs}} P(j|k)\cdot P(k)
  = \sum_{k=1}^{N_\text{srcs}} \underbrace{P(j,k)}_\text{unknown}
  \mcomma
\end{equation}
because only the conditional probability contribution $P(j|k)$ from each source $k$ per sample $j$ is known, but not the joint distribution $P(j,k)$.
% That means, the needed probability $P(j)$ of getting a signal contribution $n_S w_j$ from sample $j$ splits into $P(j|k)$ and $P(k)$.
$P(j|k)$ is the probability of getting signal from source $k$ within sample $j$, normalized over all samples
\begin{equation}
  \sum_{j=1}^{N_\text{sam}} P(j|k) = 1
  \mperiod
\end{equation}
Additionally, $P(k)$ is the probability of getting signal from source $k$ at all within any sample, separately normalized over all sources
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}} P(k) = 1
  \mperiod
\end{equation}
These relations can also be written in a concise matrix notation
\begin{equation}
  \begin{pmatrix} w_1 \\ \vdots \\ w_{N_\text{sam}} \end{pmatrix} =
    \begin{pmatrix}
      P(j=0|k=0) & \dots & P(j=0|k=N_\text{srcs}) \\
      \vdots & \ddots & \vdots \\
      P(j=N_\text{sam}|k=0) & \dots & P(j=N_\text{sam}|k=N_\text{srcs})
    \end{pmatrix} \cdot
    \begin{pmatrix}
      P(k=0) \\ \vdots \\ P(k=N_\text{srcs})
    \end{pmatrix}
  \mperiod
\end{equation}
The unnormalized, conditional signal expectation values can be obtained in each sample by calculating the expected number of events from a signal simulation which usually differs for each detector configuration and sample selection criteria.
These values can then be used to normalize the matrix per column and construct the $P(k)$ vector by summing over each column per source $k$.

The most complex weighting case in this scenario would be multiple sources that have time PDFs overlapping in their emission region and are also leaking into another data sample.
The formalism above still applies, and the sample splitting weighs can be obtained by integrating the time emission PDFs per sample to obtain the relative emission strength for the source portions lying in each sample.
These are then multiplied with the usual declination dependent weights per sample to form the $P(j|k)$ entries of the above matrix.
The weighting within each sample wouldn't be affected and is still valid as explained in the previous section.

For the special case treated here, with each source having its unique time window and also falling exclusively in a single sample, each column has only a single entry which is $1$ after the trivial normalization. \TODO{Maybe put complete and explicit matrix and weights for the 22 HESE sources in the appendix.}
The probabilities $P(k)$ can be obtained by using the un-normalized source stacking weights regardless of the sample $\tilde{w}_k$ and re-normalize them over all sources in all samples
\begin{equation}
  P(k) = \frac{\tilde{w}_k}{\sum_{m=1}^{N_\text{srcs}} \tilde{w}_m}
  \mperiod
\end{equation}
Because of the special matrix properties used here, the explicit weights $w_j$ then turn out to be global re-normalizations of the un-normalized single sample weights $\tilde{w}_k$ with
\begin{align}
  w_j
    &= \sum_{k=1}^{N_\text{srcs}} P(j|k)\cdot P(k)
    = \sum_{k=1}^{N_\text{srcs}}
      \delta_{\{k,j|T_j(t_k)\neq 0\}} \cdot
      \frac{\tilde{w}_k}{\sum_{m=1}^{N_\text{srcs}} \tilde{w}_m} \\
    &= \sum_{k=1}^{N_{j,\text{srcs}}}
      \frac{\tilde{w}_{k,j}}{\sum_{m=1}^{N_\text{srcs}} \tilde{w}_m}
  \mcomma
\end{align}
where $T_j$ is a unique rectangle function for each sample, equally used as the rectangle function utilized to describe the unique time windows per source in each sample.
$\tilde{w}_{k,j}$ are the un-normalized stacking weights per sample.

Now the numerator turns out to be exactly the per sample normalization of the per sample splitting weights, which is the sum of all weights for the subset of all sources that actually are in the sample.
The full multi-sample test statistic then reads
\begin{equation}
  -2\ln\hat{\Lambda}
  = -2\hat{n}_S +
      2\sum_{j=1}^{N_\text{sam}} \sum_{i=1}^{N_j} \sum_{k=1}^{N_\text{srcs,j}}
      \ln\left(
        \frac{\hat{n}_{S}\frac{\tilde{w}_{k,j}}{\sum_{m=1}^{N_\text{srcs}}
              \tilde{w}_m} S_{i,k,j}}
             {\Braket{\lambda_{k,j,B}} B_{i,k,j}}
        + 1
      \right)
  \mcomma
\end{equation}
where $\tilde{w}_{k,j}$ are the un-normalized weights per source with respect to the expected signal in their corresponding sample and are normalized over all un-normalized source expectations $\tilde{w}_m$ in all samples regarded.
This expression nicely demonstrates the circumstances here, namely that each source is independent of each other source and lies completely in a single data sample, so the whole underlying Likelihood fully factorizes in events, sources and samples.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %% Derivation of classci time integrated LLH used within skylab
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Time integrated Likelihood}
The standard time integrated Likelihood formula used in IceCube point source searches can also be derived from the general, extended Likelihood formula.
A different approximation than in the time dependent case is used, which takes into account the usually larger statistics in a time integrated sample as all events count and not only these in temporal coincidence with any source.

\subsection{Single sample stacking case}
Starting again from the general form
\begin{equation}
  -2\ln\hat{\Lambda}
  = -2\left(\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} +
                                      \hat{\lambda}_{k,B} -
                                      \hat{\lambda}_{k,B}^{(0)}\right) +
    2\sum_{i=1}^N \ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}\left(
          \hat{\lambda}_{k,S} S_{i,k}+\hat{\lambda}_{k,B} B_{i,k}\right)}
          {\sum_{k=1}^{N_\text{srcs}}\left(
            \hat{\lambda}_{k,B}^{(0)} B_{i,k}
          \right)}
        \right)
  \mcomma
\end{equation}
the following approximation can be used
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}} \hat{\lambda}_{k,S} + \hat{\lambda}_{k,B} \approx
    \sum_{k=1}^{N_\text{srcs}} \hat{\lambda}_{k,B}^{(0)} \approx N
  \mperiod
\end{equation}
This means the Poisson fluctuations of the sample size are neglected.
Also, the second part is usually valid if the amount of signal expected in the data is small compared to the amount of background-like events.

These approximations cancel the Poisson term in front of the sum and leaves
\begin{equation}
  -2\ln\hat{\Lambda}
  = 2\sum_{i=1}^N \ln\left(
    \frac{\sum_{k=1}^{N_\text{srcs}}\left(
          \hat{\lambda}_{k,S} S_{i,k}+\hat{\lambda}_{k,B} B_{i,k}\right)}
          {\sum_{k=1}^{N_\text{srcs}}\left(
            \hat{\lambda}_{k,B}^{(0)} B_{i,k} \right)}
        \right)
  \mperiod
\end{equation}
For the time independent part, the background distributions $B_{i,k}$ are all the same, because they only depend on each event's location and not on any source related parameters any more.
Thus the estimators and PDFs can be written as
\begin{equation}
  \hat{\lambda}_{k,B} = \frac{1}{N_\text{srcs}} \hat{\lambda}_B
  \mintertext{and} B_{i,k} = B_i
\end{equation}
and the denominator in the logarithm can be simplified to
\begin{equation}
  \frac{1}{\sum_{k=1}^{N_\text{srcs}}\left(
           \hat{\lambda}_{k,B}^{(0)}B_{i,k} \right)}
  = \frac{1}{B_i \sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,B}^{(0)}}
  = \frac{1}{N B_i}
  \mperiod
\end{equation}
Using the same argument for the background distributions in the nominator, the stacking test statistic gets
\begin{align}
  -2\ln\hat{\Lambda}
  &= 2\sum_{i=1}^N \ln\left(
    \frac{\sum_{k=1}^{N_\text{srcs}}\left(
          \hat{\lambda}_{k,S} S_{i,k}+\hat{\lambda}_{k,B} B_{i,k}\right)}
          {N B_i} \right) \\
  &= 2\sum_{i=1}^N \ln\left(
    \frac{\sum_{k=1}^{N_\text{srcs}}\left(\hat{\lambda}_{k,S} S_{i,k}\right) +
          \hat{\lambda}_B B_i}{N B_i} \right)
  \mperiod
\end{align}

Using the fixed expectation approximation from above again, the background parameter $\hat{\lambda}_B$ can be eliminated leaving $N_\text{srcs}$ free signal parameters, one for each source
\begin{align}
  -2\ln\hat{\Lambda}
  &= 2\sum_{i=1}^N \ln\left(
    \frac{\sum_{k=1}^{N_\text{srcs}}\left(\hat{\lambda}_{k,S} S_{i,k}\right) +
          \left(
            N - \sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S}
          \right) B_i}{N B_i} \right) \\
  &= 2\sum_{i=1}^N \ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}\left(\hat{\lambda}_{k,S}
              S_{i,k}\right)}{N B_i} -
      \frac{\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S}}{N} + 1
    \right)
  \mperiod
\end{align}
where in the last step the equation is slightly rearranged to show the commonly used test statistic formula in the general time integrated stacking case.

The $N_\text{srcs}$ free parameters $\lambda_{k,S}$ can again be reduced to a single signal strength parameter $n_S$ when a-priori knowledge about the source class proportions is used via
\begin{equation}
  \lambda_{k,S} = n_S \cdot w_k
  \mintertext{and}
  \sum_{k=1}^{N_\text{srcs}} w_k = 1
  \mperiod
\end{equation}
The test statistic can then be further reduced to
\begin{align}
  -2\ln\hat{\Lambda}
  &= 2\sum_{i=1}^N \ln\left(
        \frac{\hat{n}_S \sum_{k=1}^{N_\text{srcs}}\left(w_k S_{i,k}\right)}
             {N B_i} -
        \frac{\hat{n}_S \sum_{k=1}^{N_\text{srcs}}w_k}{N} + 1
      \right) \\
  &= 2\sum_{i=1}^N \ln\left(
        \frac{\hat{n}_S}{N}\left(
          \frac{\sum_{k=1}^{N_\text{srcs}}(w_k S_{i,k})}{B_i} - 1
        \right) + 1
      \right)
  \mperiod
\end{align}
Sometimes the signal sum term is abbreviated to
\begin{equation}
  S_i^\text{(tot)} \coloneqq \sum_{k=1}^{N_\text{srcs}}\left(w_k S_{i,k}\right)
\end{equation}
and only in this case with a-priori fixed weights $w_k$, the often stated \enquote{simple replacement of the single source signal term $S_i$ with the summed signal term $S_i^\text{(tot)}$} is valid
\begin{equation}
  \sum_{i=1}^N \ln\left(
      \frac{\hat{n}_S}{N}\left( \frac{S_i}{B_i} - 1 \right) + 1
    \right)
  \rightarrow
  \sum_{i=1}^N \ln\left(
      \frac{\hat{n}_S}{N}\left( \frac{S_i^\text{(tot)}}{B_i} - 1 \right) + 1
    \right)
  \mperiod
\end{equation}

\subsection{Multiple samples stacking case}
Construction of the multi sample Likelihood formula in the time integrated case is done exactly as in the time dependent case before by starting with the sample weight relation
\begin{equation}
  n_{S,j} = n_S \cdot w_j
  \mintertext{with}
  w_j = P(j) = \sum_{k=1}^{N_\text{srcs}} P(j|k)\cdot P(k)
\end{equation}
inserted in the familiar combination of single sample Likelihoods
\begin{align}
  -2\ln\hat{\Lambda}
  &= \sum_{j=1}^{N_\text{sam}} -2\ln\hat{\Lambda}_j(\hat{n}_{S,j}) \\
  &= \sum_{j=1}^{N_\text{sam}} \left[
      2\sum_{i=1}^{N_j} \ln\left(
        \frac{\hat{n}_{S,j}}{N_j}\left(
          \frac{\sum_{k=1}^{N_\text{srcs}}(w_{k,j} S_{i,k,j})}{B_{i,j}} - 1
        \right) + 1
      \right)
    \right] \\
  &= \sum_{j=1}^{N_\text{sam}} \left[
      2\sum_{i=1}^{N_j} \ln\left(
        \frac{\hat{n}_S \cdot w_j}{N_j}\left(
          \frac{\sum_{k=1}^{N_\text{srcs}}(w_{k,j} S_{i,k,j})}{B_{i,j}} - 1
        \right) + 1
      \right)
    \right]
  \mperiod
\end{align}
The number of events $N_j$ is now related to the number of events per sample $j$, the number of sources is the same in each sample, because of the steady state emission scenario.
Also the PDFs $S_{i,k,j}$, $B_{i,j}$ and stacking weights $w_{k,j}$ are taken and normalized per sample as seen before in the time dependent Likelihood.

Because all sources contribute in all times and thus all samples, the weights are not written out in a compact form here, but they still represent the relative sensitivity of a single sample to a given source hypothesis combined with the global sensitivity to a single source across all samples.
This may also be seen as a special case for the time-dependent Likelihood weights, where all the time windows for signal and background are as large as the sample livetimes.

A notable difference arises if another global fit parameter is introduced alongside the mandatory $n_S$.
This is often done to better adapt to the unknown signal hypothesis and in a time-integrated analysis, the statistics are usually sufficient to reliably fit an additional parameter.
Usually, this a parameter describing the signal flux hypothesis and is modelled as a single unbroken power law with spectral index $\gamma_k$ per source in the energy PDF term
\begin{equation}
  w_j(\vec{x}_k, \gamma_k)
  = \frac
      {
        T_j \cdot \int_{E_\nu} A_{j,\text{eff}}(E_\nu,\Omega_k)\cdot
        \phi(E_\nu, \Omega_k | \gamma_k)\,\d{E_\nu}
      }
      {
        \sum_{m=1}^{N_\text{srcs}}
        T_m \cdot \int_{E_\nu} A_{m,\text{eff}}(E_\nu,\Omega_k)\cdot
        \phi(E_\nu, \Omega_k | \gamma_k)\,\d{E_\nu}
      }
  \mperiod
\end{equation}
This leads to splitting weights that dependent on the actual shape of the assumed signal flux because the sensitivity per sample can change when the flux gets harder or softer.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %% General method to get a-priori weights from effective areas
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A-priori weight selection}
The a-priori weights for the expected signal of a source in a specific sample should match the true emission model as closely as possible.
Note that the weights resemble the expected relative flux at the detector.
For example, for an IceCube-like detector that only measures the northern sky, all weights for sources on the southern sky would be zero, regardless of the true intrinsic emission strength because no signal is ever detected.

To construct the weights, that actually distribute the total expected amount of signal to the expected contribution from each source, signal simulation can be used to estimate the signal detection efficiencies.
This is done by calculating the total number of expected events from each source from a given emission scenario at detector level.
Depending on the assumed emission scenario, the weights might depend on any source parameter, usually the source position, the time and the spectral emission behaviour of the source.
For sources, that are not point-like in any parameter but show a functional dependency, the detector response must be folded with the source parameter distribution to obtain the effective weight.

For the spatial weight contribution, the weights are dependent only on the source positions and the signal response of the detector at their positions.
The assumption that IceCube is right-ascension symmetric yields stacking weights that only depend on the source declinations.
For extended sources the, detector response needs to be folded with the source extension density profile incorporating efficiencies from multiple detector regions.

When using a time-dependent emission scenario the number of expected events do vary with the emission type.
For example in a per-burst scenario, the amount of signal events is independent from the considered emission time scale and a time-integrated fluence is assumed rather than a time-dependent flux.
Because the detector response to signal usually does not vary with time, only the relative differences in the time window functions matter for the stacking weights.
The cases regarded in this thesis both lead to a vanishing temporal dependence.
In the time-dependent analysis a fluence scenario is considered and all sources have the same time window length, making the temporal emission equal by design.
In the time-independent case, all sources have the same linear dependence on the flux
\begin{equation}
  \int_T \phi \,\d{t} = T\cdot \phi
\end{equation}
and are expected to emit for the whole sample livetime, so all integrals over the temporal part are equal.
Only when considering fluxes differential in time with different emission profiles, the integral contributions differ and the stacking weights become truly time dependent.
It is also possible to split time dependent sources across multiple sample with this formalism.
If they are assumed to depend on a flux, then the portion of the flux PDF in each sample can be computed.
If using a per-burst scenario the time evolution information is lost, so a fair approach would be to assume a uniform dependence and again see how much is in each sample.
Repeating sources in per-burst scenarios can be treated by assuming that each burst is a separate source and making sure, that time windows do not overlap.
The latter is needed, because a source can't emit two bursts at the same time.

The third common scenario is that sources also vary in their energy dependence for their intrinsic flux strengths.
Usually this happens in the for of altering the spectral index of the assumed power law flux.
This means the detector response is additionally depending on the spectral index as expected events per declination change depending on the spectrum's steepness.

To summarize, note that the source weights should represent the estimated splitting of the single signal strength fit parameter, reflecting the relative expected number of events for the whole tested parameter space and thus do not vary per event, but only per source.
It is useful to imagine what happens if the proposed source class emits signal into the detector and what would be recorded after the detection process.

%%%%%%%%
%Replaced by an a (hopefully) correct version entangling PDFs and expectations
% As an example, assuming a dependency on the sources' declinations $\delta_k$ and spectral indices $\gamma_k$ the general $n_S$ splitting weight per source $k$ would then be obtained f
% \begin{equation}
%   w_k^\text{det} = w(\vec{x}_k, t, t_k, \gamma_k)
%   = w^{S,E}(\vec{x}_k, \gamma) \cdot w^T(t, t_k)
%   \mcomma
% \end{equation}
% where $t$ can be any point in time and the spatial and spectral index dependent weights are correlated because a different index changes the declination distribution of the signal efficiency.
% The weights $w_k^\text{det}$ describe the full expected, relative detection efficiency for each source under the current data sample.
% For the time dependence, as expressions are only evaluated for the recorded events times, the weights are also only evaluated at times $t=t_i$.

% In the special case treated here, having sources unique in their time windows, the weight calculation simplifies to the spatial, source declination dependent part only, because only one source at a time has an emission expectation, so the normalized time dependent weights are $w_k^T = 1$ for each source.
% The general case would be to get the time dependent weights for each event from the time signal PDF per source and then normalizing over all contributions.
%%%%%%%%

As an example, a dependency on the source positions $\vec{x}_k$ and spectral indices $\gamma_k$ of the $n_S$ splitting weight per source $k$ is assumed.
For actually calculating the signal efficiency and thus the expected number of events per source $N_k$ at detector level and for the final sample event selection from an intrinsic, linearly time dependent neutrino flux $\phi$, the expression
\begin{equation}
  N_k(\vec{x}_k, \gamma_k)
  = T \cdot \int_{E_\nu} \int_{\Omega_\nu} f(\vec{x}_k - \vec{x})
      A_\text{eff}(E_\nu,\Omega_\nu) \cdot
      \phi(E_\nu,\Omega_\nu | \gamma_k) \,\d{E_\nu}\d{\Omega_\nu}
\end{equation}
is used.
Note the usage of the true neutrino energies and directions, as the interest lies in the mapping of intrinsic flux strength to expected events, not in the performance of spatial or energy reconstruction algorithms.
Here, the value of $\gamma_k$ is taken to be exact and for a point-source hypothesis, the function $f(\vec{x})$ can be replaced with a $\delta$ distribution for picking only the integral contribution from the single source position.
$A_\text{eff}$ is the hypothetical detector surface area which would yield the same number of observed events if it would detect $\SI{100}{\percent}$ of the incoming flux compared to the real detector and the applied event selection and maps an intrinsic flux to event counts detected at detector level.
The integrated $A_\text{eff}$ is typically in units $\si{\m\squared}$, the flux $\phi$ in \si{\per\GeV\per\m\squared\per\second\per\steradian} and the detector livetime $T$ in $\si{\second}$.
When using a time-integrated fluence $\Phi$ model the livetime is obsolete and the number of events read
\begin{equation}
  N_k \rightarrow \frac{N_k}{T}
  \mperiod
\end{equation}
The normalized weights per source in a single sample can be constructed by
\begin{equation}
  w_k = \frac{N_k}{\sum_{m=1}^{N_\text{srcs}} N_m}
  \mperiod
\end{equation}
This effective area formulation holds for calculating the multi sample splitting weights as well as for the expected $n_S$ splitting within a single sample, because it generally calculates the detector efficiency to a given flux or fluence hypothesis.

In practice to obtain the desired number of events for a given signal flux or fluence a simulation quantity called OneWeight is used to weight neutrino simulation.
The OneWeight per event $i$ is defined as \CITE{ANIS}
\begin{align}
  \text{OneWeight}_i
  &= p_\text{int}\frac{\int_\Omega\int_{E_0}^{E_1}
                       \Phi_\text{gen}(E_i)\d{E}\d{\Omega}
                       }{\Phi_\text{gen}(E_i)}A_\text{gen} \\
  &= p_\text{int}\frac{\int_{E_0}^{E_1}\Phi_\text{gen}(E_i)\d{E}}
                      {\Phi_\text{gen}(E_i)}
    A_\text{gen}\Omega_\text{gen}
  \mcomma
\end{align}
where $p_\text{int}$ is the interaction probability for neutrinos forced to interact close to the detection volume \footnote{Otherwise the simulation efficiency would be very low due to the small interaction probabilities of neutrinos.}, $\Phi_\text{gen}$ is the energy fluence used to generate the initial neutrino distribution, usually a power law and $A_\text{gen}$, $\Omega_\text{gen}$ are the surface and the solid angle over which the initial neutrinos are injected.
In the last step, the integration over $\Omega$ is carried out because usually, the injection fluence is only energy dependent.
By this definition, OneWeight is the inverse of the generating fluence in $\si{\GeV\steradian\cm\squared}$ combined with the correction factor for the forced interaction.
To obtain the number of equivalent data events for a given simulation set, the per event weights
\begin{equation}
  N = \sum_i w_i = \sum_i \frac{\Phi_i}{\Phi_{i,\text{gen}}}
\end{equation}
need to be summed up.
When using OneWeight to express the generating flux the number of events can be obtained via
\begin{align}
  N &= T \cdot \sum_i \frac{\text{OneWeight}_i \cdot \phi_i}{N_\text{gen}} \\
  \intertext{or for fluences without the livetime $T$}
  N &= \sum_i \frac{\text{OneWeight}_i \cdot \Phi_i}{N_\text{gen}}
  \mcomma
\end{align}
where $N_\text{gen}$ is the generated total number of events summed over both particle and anti-particle types.
This scaling makes sure, that the physical expectation value stays the same regardless of the number of simulated events.
The sum can also be taken across subsamples of a whole data set to obtained differential event counts, e.g. per energy or solid angle to obtain approximations for the analytic formulas shown above.

\subsection{Intrinsic source weights}
Additional intrinsic source weights can be introduced to capture differences in the expected, relative fluxes or fluences from the sources themselves.
This is decoupled from the actual detection mechanism and the detector efficiency weights described above.
The weight selection strongly depends on the used catalogue or source category and can, for example, be the gamma or x-ray flux or distance weighting \CITE{1, 2 analyses with this weighting}.
These intrinsic source weights $w_k^\text{src}$ are independently multiplied with the corresponding detector weights $w_k^\text{det}$ to form the total source weights use in the stacking
\begin{equation}
  w_k \coloneqq w_k^\text{det} \cdot w_k^\text{src}
  \mperiod
\end{equation}
If the intrinsic source properties are not known or not reliably available otherwise, the intrinsic source weights are usually assumed to be equal.

\subsection{Connecting weighting formulas}
The connection to the effective area formula from above with the more practical one using simulation weights is shortly depicted below.
From the definition of the effective area
\begin{equation}
  A_\text{eff}(\Delta E, \Delta \Omega) =
    A_\text{gen} \frac{\hat{N}(\Delta E, \Delta \Omega)}
                      {\hat{N}_\text{gen}(\Delta E, \Delta \Omega)}
  \mcomma
\end{equation}
where $\Delta E$ and $\Delta \Omega$ are arbitrary chosen integration intervals in energy and solid angle, because effective area is only properly defined under the integral.
The number of generated events per interval can be obtained from the flux or fluence assumption used to sample the primary simulation particles
\begin{equation}
  \phi_\text{gen} = \phi_0 \cdot f(E, \Omega)
  \mperiod
\end{equation}
where the unitless functional dependency is caught in the usually energy and solid angle dependent function $f(E, \Omega)$.
Because the generating function is usually uniform and independent in solid angle it follows that $f(E, \Omega) \rightarrow f(E)\cdot \sfrac{1}{\Omega_\text{gen}}$.
The number of simulated primaries in the interval is then given by
\begin{equation}
  \hat{N}_\text{gen}(\Delta E, \Delta\Omega) =
    N_\text{gen} \cdot\frac{\Delta\Omega}{\Omega_\text{gen}}
    \frac{\int_{E'}^{E'+\Delta E} f(E) \d{E}}
         {\int_{E_0}^{E_1} f(E) \d{E}}
  \mperiod
\end{equation}
For neutrino simulation where primaries are forced to interact close to the detection volume, the number of events is given by
\begin{equation}
  \hat{N}(\Delta E, \Delta \Omega) =
  \sum_{(i|E_i\in\Delta E, \Omega_i\in\Delta\Omega)} p_{i,\text{int}}
  \mperiod
\end{equation}
Now replacing $\hat{N}_\text{gen}$ and $\hat{N}$ in the effective area definition yields
\begin{equation}
  A_\text{eff}(\Delta E, \Delta \Omega) =
    A_\text{gen} \cdot
    \left(\sum_{(i|E_i\in\Delta E, \Omega_i\in\Delta\Omega)}
          p_{i,\text{int}}\right) \cdot
    \frac{\Omega_\text{gen}}{\Delta\Omega} \cdot \frac{1}{N_\text{gen}}
    \frac{\int_{E_0}^{E_1} f(E) \d{E}}
         {\int_{E'}^{E'+\Delta E} f(E) \d{E}}
\end{equation}
Identifying and replacing the OneWeight definition the formula to obtain the effective area estimate from a simulation data set reads
\begin{equation}
  A_\text{eff}(\Delta E, \Delta \Omega) =
    \frac{1}{N_\text{gen}\Delta\Omega}\cdot
    \frac{\left(
            \sum_{(i|E_i\in\Delta E, \Omega_i\in\Delta\Omega)}
            \text{Oneweight}_i \cdot f(E_i)
          \right)}
          {\int_{E'}^{E'+\Delta E}f(E)\d{E}}
  \mperiod
\end{equation}
Sometimes a slightly modified version
\begin{equation}
  A_\text{eff}(\Delta E, \Delta \Omega) =
    \frac{1}{N_\text{gen}\Delta\Omega}\cdot
    \frac{\sum_{(i|E_i\in\Delta E, \Omega_i\in\Delta\Omega)}
          \text{Oneweight}_i}
         {\Delta E}
\end{equation}
is used, which is only valid for small integration intervals, because it assumes a constant generating function $f$ within an interval by approximating
\begin{equation}
  \frac{f(E_i)}{\int_{E'}^{E'+\Delta E}f(E)\d{E}}
  \approx \frac{f(E_i)}{f(E')\cdot\Delta E}
  \approx \frac{f(E_i)}{f(E_i)\cdot\Delta E}
  = \frac{1}{\Delta E}
  \mperiod
\end{equation}
