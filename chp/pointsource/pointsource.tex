\chapter{Point source searches with IceCube neutrinos}

This chapter derives the unbinned Likelihood search methods used for IceCube point source searches and in this thesis to identify point like neutrino sources.

\needsTODO{General introduction: Signal and background contributions. Possible connection to multimessenger recap. Sample description and characterization in analysis part.}

We start from the general unbinned extended Likelihood approach and process through the special cases used here to carry out the time dependent and independent searches particularly handling multiple years of data from different detector configurations and multiple sources for the stacking case.
In the following we note a single event with index $i$, a source with index $k$ and a data sample with index $j$.

\section{Extended Likelihood}
The extended Likelihood \needsCite{Barlow book} and the corresponding logarithmic extended Likelihood function is defined as
\begin{equation}
  \mathcal{L}(\lambda) = \frac{\lambda^N e^{-\lambda}}{N!}\prod_{i=1}^N P_i
  \quad\Rightarrow\quad
    \ln\mathcal{L}(\lambda) = -\lambda+\sum_{i=1}^N \ln(\lambda P_i)
  \mcomma
\end{equation}
where the constant term $\ln(N!)$ is dropped in the logarithmic version.
Here $\lambda$ is the expected number of events and $N$ the number of measured events following a Poisson counting distribution.
The per event model distribution $P_i$ describes the Likelihood of each event under the assumed model and how likely it contributes to the expectation.

The tested hypotheses are encoded in the description of the models $P$.
To obtain a fairly general expression we can derive the point source special cases from, we can split the expectation model in multiple classes by splitting the expectation and the models accordingly:
\begin{equation}
  \lambda = \sum_{k=1}^{N_\text{classes}} \lambda_k \geq 0
  \mintertext{and}
  P_i = \frac{1}{\sum_{k=1}^{N_\text{classes}} \lambda_k}\cdot
         \sum_{k=1}^{N_\text{classes}} \lambda_k P_{i,k}
  \mperiod
\end{equation}
The single $\lambda_k$ can be negative but their sum must not because it is still a Poisson expectation parameter.
Additionally we normalized the new split model over all classes and arrive at the form
\begin{equation}
  \ln\mathcal{L}(\{\lambda_k\})
  = -\sum_{k=1}^{N_\text{classes}} \lambda_k +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{classes}}
      \lambda_k P_{i,m} \right)
  \mperiod
\end{equation}

To specialize more, we want to test a signal hypothesis against a background one for $N_\text{srcs}$ sources in general per event $i$.
We thus expand the above expression to include $N_\text{srcs}$ signal and $N_\text{srcs}$ background parameters and the corresponding distributions $S_{i,k}$ and $B_{i,k}$:
\begin{equation}
  \ln\mathcal{L}(\{\lambda_{k,S}\}, \{\lambda_{k,B}\})
  = -\sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,S}+\lambda_{k,B}\right) +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{srcs}}\left(
      \lambda_{k,S} S_{i,k}+\lambda_{k,B} B_{i,k}\right)\right)
  \mcomma
\end{equation}
and from the Poisson condition we still have the constrain
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,S}+\lambda_{k,B}\right) \geq 0
  \mperiod
\end{equation}

For testing the significance of a potential signal contribution in the measured data, a use Likelihood ratio test is used.
The null hypotheses $H_0$, which is that only background is expected to be measured, is constructed by using only a portion $\Theta_0$ of the allowed parameter space, here by setting all signal expectations to zero
\begin{equation}
  \ln\mathcal{L}_0(\{\lambda_{k,S}=0\}, \{\lambda_{k,B}\})
  = -\sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,B}\right) +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{srcs}}\left(
      \lambda_{k,B} B_{i,k}\right)\right)
  \mperiod
\end{equation}
The alternative hypothesis $H_1$ is constructed using the full Likelihood parameter space $\Theta$:
\begin{equation}
  \ln\mathcal{L}_1(\{\lambda_{k,S}\}, \{\lambda_{k,B}\})
  = -\sum_{k=1}^{N_\text{srcs}}\left(\lambda_{k,S}+
                                     \lambda_{k,B}\right) +
    \sum_{i=1}^N \ln\left(\sum_{k=1}^{N_\text{srcs}}\left(
      \lambda_{k,S} S_{i,k}+\lambda_{k,B} B_{i,k}\right)\right)
  \mperiod
\end{equation}

The Likelihood ratio test statistic $\Lambda$ for testing the null hypothesis $H_0$ against the alternative $H_1$ is defined as \needsCite{Casella Berger Book}
\begin{equation}
  \ln\Lambda = \ln\left(\frac{\sup_{\theta \in \Theta_0} \mathcal{L}(\theta)}
                          {\sup_{\theta \in \Theta} \mathcal{L}(\theta)}\right)
  = \ln\left(\sup_{\theta \in \Theta_0} \mathcal{L}(\theta)\right) -
    \ln\left(\sup_{\theta \in \Theta} \mathcal{L}(\theta)\right)
  \mperiod
\end{equation}

Here we introduce $\hat{\lambda}_{k,S/B}$ which mean the parameters $\lambda_{k,S/B}$ that maximize the Likelihood $\mathcal{L}_1$ under the complete parameter space and $\hat{\lambda}_{k,B}^{(0)}$ maximizing $\mathcal{L}_0$.
This leads to the test statistic
\begin{align}
  -2\ln\Lambda
  &= 2\ln(\mathcal{L}_1(\{\hat{\lambda}_{k,S/B}\})) -
     2\ln(\mathcal{L}_0(\{\hat{\lambda}^{(0)}_{k,B}\})) \\
  &= -2\left(\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} +
                                       \hat{\lambda}_{k,B} -
                                       \hat{\lambda}_{k,S}^{(0)}\right) +
    2\sum_{i=1}^N \ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}\left(
          \hat{\lambda}_{k,S} S_{i,k}+\hat{\lambda}_{k,B} B_{i,k}\right)}
          {\sum_{k=1}^{N_\text{srcs}}\left(
            \hat{\lambda}_{k,B}^{(0)} B_{i,k}
          \right)}
        \right)
  \mcomma
\end{align}
which has been decorated by the factor $-2$ to be compatible to Wilks' theorem \needsCite{Wilk paper and/or statistic book}.
As seen above we have to distinguish all best fit parameters from both hypotheses in general, differentiating $\hat{\lambda}_{m,B}^{(0)}$ from the null hypothesis which are not the same as $\hat{\lambda}_{k,B}$ from the composite hypothesis.

In the following sections we see how specific model choices for the signal and background distributions and approximating assumptions transform the above expression to commonly known forms in time dependent and time integrated point source analyses.

\subsection{Per event distributions}
The introduced per event distributions are similar for both the time dependent and independent Likelihoods and follow the conventions for most point source searches in IceCube \needsCite{Braun paper. Maybe both for time dep. / indep. searches.}.




\section{Time dependent Likelihood}
To test for time dependent emission we alter our Likelihood similar to what is usually called \enquote{Gamma Ray Burst Likelihood}\footnote{Named after the cited analysis, searching for emission from a Gamma Ray Burst catalog. The concept can be applied to other sources testing a similar hypothesis of emission within a defined time window.} \needsCite{Mikes thesis}.
The assumption is, that transient sources have a well defined time window over which neutrino emission might occur.
Here we additionally use non-overlapping time windows so that each source is unique in its own time window.

To account for that, the signal and background PDFs are chosen to include a time part, which is in the most simple case a rectangle function, having only a non-zero contribution at the GRB time windows:
\begin{align}
  S_{i,k}^T &= \rect\left(\frac{t_i - \frac{T_k^1-T_k^0}{2}}
                              {T_k^1-T_k^0}\right) \\
  B_{i,k}^T &= \rect\left(\frac{t_i - \frac{T_k^1-T_k^0}{2}}
                              {T_k^1-T_k^0}\right)
  \mcomma
\end{align}
which will be used later.

The other important simplification is that we don't fit the background expectations, but rather fix them from the integrated off-time data rate over the range of the background time PDFs.
This decreases the number of parameters to fit for, unifies the background estimators
\begin{equation}
  \hat{\lambda}_{k,B} = \hat{\lambda}_{k,B}^{(0)} = \Braket{\lambda_{k,B}}
\end{equation}
and the test statistic turns to
\begin{equation}
  \frac{1}{2}\Lambda
  = -\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} +
    \sum_{i=1}^N \ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} S_{i,k}}
           {\sum_{k=1}^{N_\text{srcs}}\Braket{\lambda_{k,B}} B_{i,k}}
      + 1 \right)
  \mperiod
\end{equation}

In the following we adapt the notation to the standard and use $n$ instead of $\lambda$.


\subsection{Single sample stacking case}

The stacking GRB test statistic for a single sample now has the form
\begin{align}
  \frac{1}{2}\Lambda
  = -\hat{n}_S + \sum_{i=1}^N \ln\left(\frac{\hat{n}_S\bar{S}_i}{\Braket{n_B} B_i} + 1\right) \mcomma
\label{equ:TS}
\end{align}
where $\hat{n}_S$ is the best fit $n_S$ under the full parameter space and $\bar{S}_i$ the stacked signal PDFs per event at each source $k$
\begin{equation}
  \bar{S}_i = \sum_{k=1}^{N_\text{srcs}} w_k^\text{D} w_k^\text{T} S_{i,k} \mperiod
\end{equation}
The detector weights $w^\text{D}$ and intrinsic source weights $w^\text{T}$ are normalized such that
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}} w^\text{D}_k w^\text{T}_k = 1 \mcomma
\end{equation}
where it doesn't matter if $w^\text{D}$ or $w^\text{T}$ have been normalized on their own or not.

For a single sample and assuming non-overlapping GRB time windows, we can move the sum over the signal PDFs in eq.~(\ref{equ:TS}) outside the logarithm
\begin{align}
  \frac{1}{2}\Lambda
    &= -\hat{n}_S + \sum_{i=1}^N \ln\left(\frac{\hat{n}_S\bar{S}_i}
                                               {\Braket{n_B} B_i} + 1\right) \\
    &= -\hat{n}_S + \sum_{i=1}^N \ln\left(
          \frac{\hat{n}_S\sum_{k=1}^{N_\text{srcs}}
                w_k^\text{D}w_k^\text{T}S_{i,k}}
               {\Braket{n_B} B_i} + 1\right) \mcomma \\
  \intertext{utilizing that each GRB has it's unique time window, we see that each event $i$ can only contribute to a single GRB, so that}
    &= -\hat{n}_S + \sum_{i=1}^N \ln\left(
          \frac{\hat{n}_S\sum_{k=1}^{N_\text{srcs}}
                w_k^\text{D}w_k^\text{T}S_{i,k}\delta_{i\in k}}
               {\Braket{n_B} B_i} + 1\right) \mcomma \\
  \intertext{where $\delta_{i\in k}$ is $1$ only if event $i$ falls into the region of GRB $k$, otherwise $0$, so we get}
    &= -\hat{n}_S + \sum_{i=1}^N \ln\left(
          \frac{\hat{n}_S
                \left[0 + \dots + 0 +
                      w_k^\text{D}w_k^\text{T}S_{i,k} +
                      0 + \dots + 0\right]}
               {\Braket{n_B} B_i} + 1\right) \mperiod \\
  \intertext{Because $\ln(0+1) = \ln(1) = 0$ we can use this to move the sum outside the logarithm}
    &= -\hat{n}_S + \sum_{k=1}^{N_\text{srcs}}\sum_{i=1}^N \ln\left(
          \frac{\hat{n}_S w_k^\text{D}w_k^\text{T}S_{i,k}}
               {\Braket{n_B} B_i} + 1\right)  \label{equ:single_stack} \\
  \intertext{which, as a crosscheck, expands to the correct expression again:}
    &= -\hat{n}_S + \sum_{i=1}^N \ln\left[
          \ln(1) + \dots + \ln(1) +
          \left( \frac{\hat{n}_S w_k^\text{D}w_k^\text{T}S_{i,k}}
                      {\Braket{n_B} B_i} + 1\right) +
          \ln(1) + \dots + \ln(1) \right] \\
    &= -\hat{n}_S + \sum_{i=1}^N \ln\left[ \left(
          \frac{\hat{n}_S w_k^\text{D}w_k^\text{T}S_{i\in k}}
               {\Braket{n_B} B_i} + 1\right) \right] \mperiod
\end{align}

In the following we'll see, that eq.~(\ref{equ:single_stack}) is already the form we can use to stack over multiple samples.


\subsection{Multiple samples stacking case}

Now if we add another sample, we can just sum up the individual Likelihoods, because they are independent:
\begin{align}
  \frac{1}{2}\Lambda
    &= \frac{1}{2} \sum_{j=1}^{N_\text{sam}} \Lambda_j(\hat{n}_{S,j}) \\
    &= \sum_{j=1}^{N_\text{sam}} \left[
        -\hat{n}_{S,j} + \sum_{k\in j}\sum_{i=1}^{N_j} \ln\left(
          \frac{\hat{n}_{S,j} w_{k,j}^\text{D}w_{k,j}^\text{T}S_{i,k,j}}
               {\Braket{n_B} B_{i,j}} + 1
        \right)
      \right] \\
    &=  -\hat{n}_S + \sum_{j=1}^{N_\text{sam}}
          \sum_{k\in j}\sum_{i=1}^{N_j} \ln\left(
            \frac{\hat{n}_S w_j w_{k,j}^\text{D}w_{k,j}^\text{T}S_{i,k,j}}
                 {\Braket{n_B} B_{i,j}} + 1
        \right) \mperiod
\label{equ:multi_TS}
\end{align}
Here we have individual $n_{S,j}$ signal parameters and GRBs can exclusively fall in only one of the $N_\text{sam}$ samples.
This is indicated by the sum $\sum_{k\in j}$ over GRBs now only running over all GRBs included in sample $j$ and the number of events are now the numbers $N_j$ per sample.
Also the PDFs $S_{i,k,j}$ and $B_{i,j}$ are taken with respect to the sample the source and corresponding events fall into because the PDFs can change depending on the event selection, etc.
Finally, $w_{k,j}^\text{D} w_{k,j}^\text{T}$ are now normalized separately within the sample, so that
\begin{equation}
  \sum_{k\in j} w_{k,j}^\text{D} w_{k,j}^\text{T}
  = \sum_{k\in j} \hat{w}_k^\text{D} \hat{w}_k^\text{T} \cdot
    \frac{1}{\sum_{m\in j} \hat{w}_m^\text{D} \hat{w}_m^\text{T}}
  = 1
  \mperiod
  \label{equ:multi_norm}
\end{equation}

In the last step in eq.~(\ref{equ:multi_TS}) we introduced a weight $w_j$ with
\begin{equation}
  \sum_{j=1}^{N_\text{sam}} w_j = 1 \mcomma
\end{equation}
that scales $\hat{n}_S$ to $\hat{n}_{S,j} = \hat{n}_S w_j$.
We need this to insert the correctly scaled single sample LLH parameters $\hat{n}_{S,j}$ which are in general smaller than the combined $\hat{n}_S$.

To obtain the weights $w_j$ we use the law of total probability
\begin{equation}
  w_j = P(j) = \sum_{k=1}^{N_\text{srcs}} P(j| k)\cdot P(k) \mcomma
\label{equ:tot_prob}
\end{equation}
where $P(j)$ is the probability of getting a signal contribution $\hat{n}_S w_j$ from sample $j$.
$P(j|k)$ is the probability of getting signal from source $k$ from sample $j$, normalized over all samples:
\begin{equation}
  \sum_{j=1}^{N_\text{sam}} P(j|k) = 1 \mperiod
\end{equation}
$P(k)$ is the probability of getting signal from source $k$ at all, normalized over all sources separately:
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}} P(k) = 1 \mperiod
\end{equation}
While eq.~(\ref{equ:tot_prob}) holds in more general cases we can use the assumption about our non-overlapping GRBs to simplify the expression.
We use matrix notation for eq.~(\ref{equ:tot_prob}) to better illustrate that:
\begin{equation}
  \vec{w} =
    \begin{pmatrix}
      P(j=0|k=0) & \dots & P(j=0|k=N_\text{srcs}) \\
      \vdots & \vdots & \vdots \\
      P(j=N_\text{sam}|k=0) & \dots & P(j=N_\text{sam}|k=N_\text{srcs})
    \end{pmatrix} \cdot
    \begin{pmatrix}
      P(k=0) \\ \vdots \\ P(k=N_\text{srcs})
    \end{pmatrix} \mperiod
\end{equation}

Because each GRB is assumed to be non-overlapping, each column can only have a single entry which is $1$ after normalization per column.
Each row may contain several entries which in sum are then the number of GRBs per sample.
The probabilities $P(k)$ can be obtained from the detection efficiency of the detector at the GRB position in the sample they fall into and are normalized over all sources (see eq.~(\ref{equ:multi_example}) for an example):
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}} P(k)
    = \sum_{k=1}^{N_\text{srcs}} w^\text{D}_k w^\text{T}_k \cdot
      \frac{1}{\sum_{m=1}^{N_\text{srcs}} w^\text{D}_m w^\text{T}_m}
    = 1
  \mperiod
\end{equation}

Because the matrix $P(j|k)$ has only entries with either $1$ or $0$, the weights $w_j$ are effectively the sum over the detection efficiencies of all GRBs falling in sample $j$
\begin{equation}
  w_j = \sum_{k\in j} w^\text{D}_k w^\text{T}_k
        \frac{1}{\sum_{m=1}^{N_\text{srcs}} w^\text{D}_m w^\text{T}_m}
      \mperiod
\end{equation}
$w^\text{D}_k$ and $w^\text{T}_k$ must not be confused with the weights $w_{k,j}^\text{D}$ and $w_{k,j}^\text{T}$ which are normalized per sample $j$.

But the numerators of the weights $w_j$ are now exactly the normalization of the $w_{k,j}^\text{D}$, $w_{k,j}^\text{T}$ in each sample, as previously seen in eq.~(\ref{equ:multi_norm}):
\begin{equation}
  \text{norm}(\{w_{k,j}^\text{D}w_{k,j}^\text{T}\})
    = \sum_{k\in j} w^\text{D}_k w^\text{T}_k
    = w_j \cdot \sum_{k=1}^{N_\text{srcs}} w^\text{D}_k w^\text{T}_k
  \mperiod
\end{equation}

So if we plug the $w_j$ back in to the test statistic (\ref{equ:multi_TS}) we get
\begin{align}
  \frac{1}{2}\Lambda &=
    -\hat{n}_S + \sum_{j=1}^{N_\text{sam}}
            \sum_{k\in j}\sum_{i=1}^{N_j} \ln\left(
              \frac{\hat{n}_S w_j w_{k,j}^\text{D}w_{k,j}^\text{T}S_{i,k,j}}
                   {\Braket{n_B} B_{i,j}} + 1 \right) \\
    &= -\hat{n}_S + \sum_{j=1}^{N_\text{sam}}
            \sum_{k\in j}\sum_{i=1}^{N_j} \ln\left(
              \frac{\hat{n}_S \left[
                      \frac{\sum_{l\in j} w^\text{D}_l w^\text{T}_l }
                           {\sum_{m=1}^{N_\text{srcs}} w^\text{D}_m w^\text{T}_m}
                      \right] w_{k,j}^\text{D}w_{k,j}^\text{T}S_{i,k,j}}
                   {\Braket{n_B} B_{i,j}} + 1 \right)
       \mperiod \\
  \intertext{The numerator of $w_j$ now cancels the per sample normalization of $w_{k,j}^\text{D}$ and $w_{k,j}^\text{T}$ and we get}
    &= -\hat{n}_S + \sum_{j=1}^{N_\text{sam}}
            \sum_{k\in j}\sum_{i=1}^{N_j} \ln\left(
              \frac{\hat{n}_S \left[
                      \frac{ w_k^\text{D}w_k^\text{T} }
                           {\sum_{m=1}^{N_\text{srcs}} w^\text{D}_m w^\text{T}_m}
                      \right] S_{i,k,j}}
                   {\Braket{n_B} B_{i,j}} + 1 \right)
\end{align}

If we now apply the same arguments about mutually exclusive GRBs as done in the first chapter, we can combine the sums over $j$ and $k$ and also just sum over all combined events in all samples $i$.
We then get the test statistic for multiple samples and multiple sources as
\begin{equation}
  \frac{1}{2}\Lambda
  = -\hat{n}_S + \sum_{k=1}^{N_\text{srcs}}\sum_{i=1}^{N} \ln\left(
        \frac{\hat{n}_S \left[
                \frac{ w_k^\text{D}w_k^\text{T} }
                     {\sum_{m=1}^{N_\text{srcs}} w^\text{D}_m w^\text{T}_m}
                \right] S_{i,k}}
             {\Braket{n_B} B_{i, k}} + 1 \right)
   \mcomma
   \label{equ:multi_stack}
\end{equation}
where the index $k$ now means that we have to evaluate the PDFs and the weights with the configuration for the specific sample the GRB is belonging to.

\subsection{Summary}
We have explored two ways to construct a stacking GRB LLH with multiple samples:
\begin{enumerate}
  \item Construct the single sample LLHs normalizing all weights as shown in the first chapter in combination with the multiply LLH in eq.~(\ref{equ:multi_TS}).
  Then we need to introduce sample weights $w_j$ for $\hat{n}_s$, so that
  \begin{equation}
    w_j = \sum_{k\in j} w^\text{D}_k w^\text{T}_k
          \frac{1}{\sum_{m=1}^{N_\text{srcs}} w^\text{D}_m w^\text{T}_m}
  \end{equation}
  These weights re-normalize the per sample weights so they are regarding different efficiencies across samples.

  \item Alternatively we can also use form eq.~(\ref{equ:single_stack}), which is exactly the same as the later derived multi year form eq.~(\ref{equ:multi_stack}).
  For these we can simply get all our detector weights for the corresponding year and also need to use the correct PDFs for each source.
  Then the weights are normalized over all samples.
\end{enumerate}


% \subsection{How \lstinline|grbllh| does it}
% In \lstinline|grbllh| we do something similar to eq.~(\ref{equ:multi_stack}): \begin{equation}
%   \frac{1}{2}\Lambda
%   = -\hat{n}_S + \sum_{k=1}^{N_\text{srcs}}\sum_{i=1}^{N} \ln\left(
%         \frac{\hat{n}_S S_{i,k}}{\Braket{n_B} B_{i,k}} + 1 \right)
%    \mcomma
% \end{equation}
% which means we insert the total sum of all signal expectations $\hat{n}_S$ and the sum of all background expectations $\Braket{n_B}$.
% This has the same effect as using
% \begin{equation}
%   \frac{1}{2}\Lambda
%   = -\hat{n}_S + \sum_{k=1}^{N_\text{srcs}}\sum_{i=1}^{N} \ln\left(
%         \frac{\hat{n}_S w_k S_{i,k}}{\overline{\Braket{n_B}} B_{i, k}} + 1 \right)
%    \mcomma
% \end{equation}
% where all the signal weights are
% \begin{equation}
%   w_k = 1/N_\text{srcs}
% \end{equation}
% and
% \begin{equation}
%   \overline{\Braket{n_B}} =
%   \frac{1}{N_\text{srcs}}\sum_{k=1}^{N_\text{srcs}}\Braket{n_{B,k}}
%   = \frac{\Braket{n_B}}{N_\text{srcs}}
% \end{equation}
% means the mean background from all source time windows.
% So we simply use a special weighted case, where all GRBs are treated the same with respect to signal and background expectations.


% \subsection{Example Weight Matrix}
% An arbitrary example with 3 samples and 5 sources in total would be:
% \begin{align}
% \vec{w} =
%   \begin{pmatrix}
%     w_{j=0} \\ w_{j=1} \\ w_{j=2}
%   \end{pmatrix} &=
%     \begin{pmatrix}
%       0 & 0 & 1 & 0 & 0 \\
%       0 & 1 & 0 & 0 & 0 \\
%       1 & 0 & 0 & 1 & 1
%     \end{pmatrix} \cdot
%     \begin{pmatrix}
%       w_0^\text{D} w_0^\text{T} \\
%       w_1^\text{D} w_1^\text{T} \\
%       w_2^\text{D} w_2^\text{T} \\
%       w_3^\text{D} w_3^\text{T} \\
%       w_4^\text{D} w_4^\text{T}
%     \end{pmatrix} \cdot
%     \frac{1}{\sum_{k=1}^{N_\text{srcs}} w^\text{D}_k w^\text{T}_k} \\
%     &= \begin{pmatrix}
%         w_2^\text{D} w_2^\text{T} \\
%         w_1^\text{D} w_1^\text{T} \\
%         w_0^\text{D} w_0^\text{T} + w_3^\text{D} w_3^\text{T} +
%         w_4^\text{D} w_4^\text{T}
%       \end{pmatrix} \cdot
%       \frac{1}{\sum_{k=1}^{N_\text{srcs}} w^\text{D}_k w^\text{T}_k}
%     \mperiod
%   \label{equ:multi_example}
% \end{align}

\section{Time integrated Likelihood}

\section{Single sample stacking case}
Here we show, that the standard PS stacking case for a single sample emerges from the same extended LLH principle as in the GRB LLH case.
We start again with the general test statistic
\begin{equation}
  \frac{1}{2}\Lambda
  = -\left(\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} +
                                      \hat{\lambda}_{k,B} -
                                      \hat{\lambda}_{k,S}^{(0)}\right) +
    \sum_{i=1}^N \ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}\left(
          \hat{\lambda}_{k,S} S_{i,k}+\hat{\lambda}_{k,B} B_{i,k}\right)}
          {\sum_{m=1}^{N_\text{srcs}}\left(
            \hat{\lambda}_{m,B}^{(0)} B_{i,m}
          \right)}
        \right)
  \mperiod
\end{equation}

For the time independent case we have a lot more background than signal events for a whole year and we may approximate
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}} \lambda_{k,S} + \lambda_{k,B} \approx
  \sum_{k=1}^{N_\text{srcs}} \lambda_{k,B}^{(0)} \approx N
\end{equation}
so we neglect the small fraction of signal events in the Poisson fluctuation of the sample.
This cancels the terms in front of the event sum and leaves us with
\begin{equation}
  \frac{1}{2}\Lambda
  = \sum_{i=1}^N \ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}\left(
          \hat{\lambda}_{k,S} S_{i,k}+\hat{\lambda}_{k,B} B_{i,k}\right)}
          {\sum_{m=1}^{N_\text{srcs}}\left(
            \hat{\lambda}_{m,B}^{(0)} B_{i,m}
          \right)}
        \right)
  \mperiod
\end{equation}

The background PDF in the standard PS case only dependents on the event location regardless of the source positions, because the background is assumed to come diffusely from the atmosphere.
This means all background estimators are the same for each source per event
\begin{equation}
  \hat{\lambda}_{k,B} = \frac{1}{N_\text{srcs}} \hat{\lambda}_B
  \quad\text{and}\quad
  B_{i,k} = B_i
\end{equation}
and we can simplify the test statistic to
\begin{equation}
  \frac{1}{2}\Lambda
  = \sum_{i=1}^N\ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} S_{i,k} +
            \hat{\lambda}_{B} B_i}
           {N B_i}
    \right)
  \mperiod
\end{equation}

For the last step we make sure, that our Poisson condition stays satisfied by demanding
\begin{equation}
  \sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S} + \hat{\lambda}_B \approx N
  \Leftrightarrow
  \hat{\lambda}_B = N - \sum_{k=1}^{N_\text{srcs}}\hat{\lambda}_{k,S}
\end{equation}

where we also used the standard PS large sample approximation from above.
With this we can cancel the $N$ and rename $\lambda$ to $n$ and see the familiar standard PS stacking formula:
\begin{align}
  \frac{1}{2}\Lambda
  &= \sum_{i=1}^N\ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}n_{k,S} S_{i,k} +
            \left(N-\sum_{k=1}^{N_\text{srcs}}n_{k,S}\right) B_i}
           {N B_i}
    \right)  \\
  &= \sum_{i=1}^N\ln\left(
      \frac{\sum_{k=1}^{N_\text{srcs}}n_{k,S} S_{i,k}}
           {N B_i} -
      \frac{\sum_{k=1}^{N_\text{srcs}}n_{k,S}}{N} + 1
    \right)
  \mperiod
\end{align}

Sometimes we say, simply replace the signal term with a weighted sum over all signal classes to get the stacking case.
This is only possible in the case of only fitting a single $n_S$ parameter and fixing some weights $w_k$, splitting $n_{k,S} = n_S w_k$ with
\begin{equation}
  \sum_{k=1}^{N\text{srcs}} w_k = 1
\end{equation}
a-priori.
Then we get the following form
\begin{align}
  \frac{1}{2}\Lambda
  &= \sum_{i=1}^N\ln\left(
      \frac{n_S}{N}\left(
        \frac{\sum_{k=1}^{N_\text{srcs}}w_k S_{i,k}}{B_i} - 1
      \right) + 1
    \right)  \\
  &= \sum_{i=1}^N\ln\left(
      \frac{n_S}{N}\left(\frac{S_i^\text{tot}}{B_i} - 1\right) + 1
    \right)
  \mperiod
\end{align}

\section{Multiple samples stacking case}
Generalize the GRB weights here, as done by mhuber.
